#+TITLE: ID\_271\_advanced\_regression

* ID 271 - Advanced Regression: Env Epi

** My notes

*** lecture 1

Residuals=error=e1 =length of the point observed to the point expected
in the linear line
 Expected=predicted
  
 Max likelihood- max likelihood my minimizing the sum square of the
errors
  
 Least square reg is max likelihood provided that:
 Gaussian distribution
 Independence
 Constant variance
  
 Least squre reg is robust, u could relax the above assumptions a bit.

*** splines

Loess smoothing- smooth the scatter plot and get the idea of whats going
on in each "bin" or part of the scatter plot. Good to checko linearity
or other relationship.
 Splines are used instead of dummy variables for fitting a line with
multiple "pieces" through the data
 How to fit splines slide
 B1-the slope of the people up to 40
 B1+B1- the slope of the people above 40
 B2-Change of the slope
 Knot points-points where slope changes
 R lab part:
 To upate the model you can use the update function
 Model1< update(model2, .~.+VARNEW)
 Where . is the prev DV~ and . are all the previous IV's
 Putting as.factor (VARIABLE) is used for categorical variables (see
example in handout)
 Plot.density fits a smooth to the histogram

*** lecture.3

MARS- look up in Wikipedia for details- multivariate regression spline

[[http://en.wikipedia.org/wiki/Multivariate_adaptive_regression_splines][http://en.wikipedia.org/wiki/Multivariate\_adaptive\_regression\_splines]]
 in R the package is called 'earth'
 bs is a b spline in R, they are cubic splines
 ns are natural splins in R- linear at the ends
 Example: bs(age,df=5)
 5 df is for: age,age2, age3 and you are left with 2 knots
 If you have 2 knots it will take the 1st and 3rd tertilem if you had 1
it would use the median, if you had 3 knots it puts it at the quartiles
(25%,50%,75%)
 Smoothing splines are something completely different since they are non
parametric
 Smoothing splines puts a knot in every value in your variable

*** GLM

gaussian is the default, its a normal distributaion l
 poison is for count data!! (0,1,2,3....)
 in glm you can use only ns
 in gam you can use penalized splines which can choose the df for you
(using gcv) and that way you dont have to know if the relationship is
linear or not
 gcv is not always right, always plot the modle to see the results:
 IE:
 plot(model)
 Vp- is used to extract the covarience matrix for the gam model

*** time series

time series looks for daily correlation between the outcome (IE deaths)
and daily pollution (IE PM)
 in addition there are also seasonal diffrences between seasons
 using time series you also take care of confounding between the
pollution and other included variables (diet etc..)
 arima? is an old way to preform ts.
 if threre is a large number if people dying a day (~300) then you can
treat it as continous data
 in *the majority* cases you dont get that much deathes so you treat it
as *count* data
 the goal of time series studies is to remove the correlation between
the main exposure and the other confounders
 you do that by:
 design (time invarient facotrs) age dosent change by time
 filtering (slowly cahnging factors) body mass changes slowly by time
 there are still potential confounders thay flucuate from day to day
with the main exposure (pollution):
 weater,other pollution,sesonal patterns
 usually to control for season people use 4-5 DF per year
 Harvesting- a term used to say that people would die anyway from XXX
but exposure to pollution just speeds things up
 *in posion the mean and variance has to be the same, if they are not
you need to use quasi-poison for count data*

**** distributed lag modles

the problem with putting all lags is that each lag (day, etc) is
correlated which one another.
 to deal with that you need to constraint the lag data
 one example is taking the mean of the lags

*** case crossover

saves time effort and money
 but can only look at the effects of short term exposures, not chronic
effetcs
 we control for time of the day, day of the week ,season etc.. by
*matching
*for example you can control for season by choosing controls in the same
month of same year of death
 *the control days are choosen before and after (the actuall death)
*this is done to control for time trends (long term) such as air
pollution or ciggarte smoking that tend to go down so IE exposre in the
control prior to death is greater then the day od death. (see joel's
gprah form lecture)

*** case only approach

only focused on effect modeifaction that is if some facotr (ie diabites)
modefies the effect of air pollution
 again it works only with short-term exposures

*** gam

In R it uses mgcv for the penelized splines
 for penalized splines use:
 s(VAR)
 sp=XXX is for the number of lamda's. lambda is a continous variable
 the xxx is the gcv score which is the cross validation of mgcv to
determine the best number of nodes to use

** Workshops

*** workshop 1

[[ID_271_advanced_regression_files/attach/Course%20Handouts%202011%20-%20Workshop%201.doc][Attachment
#01 (Course Handouts 2011 - Workshop 1.doc)]]

*** workshop 2

[[ID_271_advanced_regression_files/attach/Course%20Handouts%202010%20-%20Workshop%202.doc][Attachment
#02 (Course Handouts 2010 - Workshop 2.doc)]]
 [[ID_271_advanced_regression_files/attach/kloog_workshop2.r][Attachment
#03 (kloog\_workshop2.r)]]
 if elsecommand from blue hill has goos example

*** workshop 3

[[ID_271_advanced_regression_files/attach/Course%20Handouts%202010%20-%20Workshop%203.doc][Attachment
#04 (Course Handouts 2010 - Workshop 3.doc)]]
 [[ID_271_advanced_regression_files/attach/kloog_workshop3.r][Attachment
#05 (kloog\_workshop3.r)]]

*** workshop 4

[[ID_271_advanced_regression_files/attach/Course%20Handouts%202009%20-%20Workshop%204.doc][Attachment
#06 (Course Handouts 2009 - Workshop 4.doc)]]
 [[ID_271_advanced_regression_files/attach/kloog_workshop4.r][Attachment
#07 (kloog\_workshop4.r)]]

*** workshop 5

[[ID_271_advanced_regression_files/attach/Course%20Handouts%202010%20-%20Workshop%205.doc][Attachment
#08 (Course Handouts 2010 - Workshop 5.doc)]]

*** workshop 6

[[ID_271_advanced_regression_files/attach/Course%20Handouts%202010%20-%20Workshop%206.doc][Attachment
#09 (Course Handouts 2010 - Workshop 6.doc)]]
 [[ID_271_advanced_regression_files/attach/kloog_workshop6.r][Attachment
#10 (kloog\_workshop6.r)]]
 First we create a subset data set without missing values. We need to
add the variable locode too.
 names(hanes1b)
 #creates a vector from the whole dataset where the numbers are for the
variables position
 ind<-c(2,3,4,5,7,11,12,14,15,20,21,22)
 #this will create a dataset based on the whole hanes dataset with only
the variables we choose in the ind vector
 hanesna<-hanes1b[ind]
 #deleted the missing from the dataset
 hanesna<-na.omit(hanesna)
 names(hanesna)
 dim(hanesna)
 dim(hanes1b)
 attach(hanesna)

*** workshop 7 quantile

[[ID_271_advanced_regression_files/attach/Quantile%20regression%20R.docx][Attachment
#11 (Quantile regression R.docx)]]
