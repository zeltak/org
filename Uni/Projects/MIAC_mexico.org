#+TITLE: MEXICO_PM_1x1

* 3.1.2-MID_AOD

** Script RUN order

** 1.Intro

*** 1.modis info
Modis files are MIAC files from alexi at 1x1km resolution
** 2.Gather data
*** 1.create daily aod data
**** 1.obtain data
AOD data was obtained through MAIAC 1x1k satellites

The lat-lon is the upper-left corner of the pixel. The projection is in MODIS projection, it is sinusoid, but not exactly in WGS84 datum,
the  earth radii is 6371007.181 meter, central median is 99W.  Upper left corner EAST = -75000 meter, North=2250000 meter.
**** 2.process data
***** Matlab
AOD data was imported using matlab.
We received RAW hdf files and a corresponding lat-long file (binary) see [[*1.obtain%20data][1.obtain data]] section for details.
we then used matlab scripts [[file:~/My%20Documents/My%20Dropbox/Org/Uni/Guides/matlab.org::*Run%20code%20MIAC%20PM%20data][Run code MIAC PM data]] to extract the hdf
data to seperate txt files.
***** SAS
We then use [[file:f:/Uni/Projects/P031_MIAC_MEXICO/2.Code/2.Gather_data/txtlst_v2.sas][SAS Code]]:
f:\Uni\Projects\P031_MIAC_MEXICO\2.Code\2.Gather_data\txtlst_v2.sas
to:

1.Combine all years into one file and clean the data
2. export a full grid
3.clip the grid to just the study area
this is done by exporting the full grid in the script with a aod
gridid (aodid), importing in GIS and selecting the  points inside the
study area and assigning them a value of 1('inside mex city') in the
clipmexcity variable

[[file:1.Images/20052013w1.png]]
then we export that back again here

file:f:\Uni\Projects\P031_MIAC_MEXICO\3.Work\2.Gather_data\FN007_Key_tables\fullRAWgrid_with_clipID.dbf

and clip the big RAW aod dataset by the clipmexcity variable.

this is exported as the final grid we use

file:f:\Uni\Projects\P031_MIAC_MEXICO\3.Work\2.Gather_data\FN007_Key_tables\fullCLIPEDgrid.dbf

then we export mod2 files (we delete non aod avilable days) here:

file:f:\Uni\Projects\P031_MIAC_MEXICO\3.Work\2.Gather_data\FN010_mod2_files\mod2.dbf

we also create abd export a full grid (day x,y) for mod3:

file:f:\Uni\Projects\P031_MIAC_MEXICO\3.Work\2.Gather_data\FN011_mod3_files\allmod3.dbf

*** 2.daily pm
**** 1.Sources
PM data was obtained through ALLAN Just..more info

*** 3.Join AOD-PM-MET-LU data
**** Create a subset of AOD relevant to stations (within X km if stations)
1) we import the PM stations (aggregated)

`F:\Uni\Projects\P031_MIAC_MEXICO\3.Work\2.Gather_data\$GIS Repo\New File Geodatabase.gdb`

2) we then select by location only the points within X distance from the monitor stations (1.5km)

%NOTE: depending on what we decide we define a search distance (1.5km
for mexico city pm)%

[[file:1.Images/2542013w6.png]]


this selection is then exported to a shapefile with only selected
features and added to the workspace.
then the we join it to the PM stations file to get the sitecode and distance

LST_within1km_met

[[file:1.Images/2542013w7.png]]


this is exported to dbf here:

file://f:\Uni\Projects\P031_MIAC_MEXICO\3.Work\2.Gather_data\FN007_Key_tables\AOD_within1km_stn.dbf

**** use SAS to create mod files


*** 4.Spatial data (Land use-emissions)
**** 1.1x1km LU
***** 0.Land use data Sources
collected by allan just
***** 2.Dist to road

the guid layer (gn1) was loaded and the A1 layer raw data was loaded
[[MIA_PM_10x10_files/img/SS-2011-04-07_11.44.53.png]]
using spatial join each guid point got the distance from the closet
road (*gn\_lu\_s1*)
 this was then multipled by 100 to get it in km
 [[MIA_PM_10x10_files/img/SS-2011-04-07_11.43.12.png]]

***** 3.Elevation
a raster image from Allen was used for eleveation. the base
resolution was resampled to 1x1km using block statistics

[[file:1.Images/gis10052003p1.png]]

%We made sure to the rasters were projected first (UTM) so we could define a precise 1x1km square%



**** 2.50x50 local LU

***** 0.intro

NOTE:make sure all layers are projected in UTM before starting to avoid
issues
 we can make use of the exact geocodes by doing the following. Take the
residuals from our model for the grid cells with monitors. Regress them
against land use terms local to the monitor (distance to road, pop
density within 300 m, land use within 300 m). Hopefully, this will
explain something, and we can then get those terms for our Mass
mortality and births, but also the NAS, Framingham and VIVA cohorts, and
get better predictions.
 All LU data was obtained through steve (see 2.1)
 all outputs are located here:

*c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.1.NE\_PM\_MODELS\3.1.1.4.Work\2.Gather\_data\AGIS\_DB\NE\_LU50x50.gdb*

***** population density

Population density was added to the point layer of monitoring stations:
 1)first we used the extract values to points. the raster values were
already the population density
 [[MIA_PM_10x10_files/img/SS-2011-06-22_10.13.09.png]]
 [[MIA_PM_10x10_files/img/SS-2011-06-22_10.16.05.png]]
 *
*

***** percent urban

****** Raw data and calculation

This shows how steve calculated the percent urban variable
 1) first a source raster image was downlaoded from the national land
cover dataset (through steve) at a resolution of 30x30 meters.
 This raster was loaded to GIS
 *
*2)**then using the reclassify tool in spatial analyst we recallified
the raster into:
 0-open space
 1-urban areas
 using the available raster values (22,23,24 were the corresponding NLCD
values for urban areas)
 this was exported to a *new raster* located here:

*C:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.2.MIA\_PM\_MODELSV3\3.1.1.1.Raw\_data\GIS\nlcd\dev01ma
 [[MIA_PM_10x10_files/img/Image.png]]*
 3) then focal statistics were used. Any cell center encompassed by the
circle will be included in the processing of the neighborhood
 [[MIA_PM_10x10_files/img/Image_gSme8PmOBx6xPoPIinQwYg_0001.png]]
 in statistics type we choose sum to get all cells with the urban value
in them
 we choose the 3, HxW to get the surrounding 9 cells.
 4)that raster obtained:
 [[MIA_PM_10x10_files/img/Image_gSme8PmOBx6xPoPIinQwYg_0002.png]]
 had the value of the sum of all urban areas (the '1' values) in each 30
meter cell we defined before

****** Add to LU data

*NOTE: since Each variable is called RASTERVALUE, gis wont be able to
extract values for another variable with the same name (it will give all
values a NULL value). To address this simple create a new variable ,
copy the raster value to it and delete the raster value field
 [[MIA_PM_10x10_files/img/SS-2011-06-22_12.56.54.png]]
*
 We use extract value to points to get the raster value in each 50 meter
grid cell
 [[MIA_PM_10x10_files/img/Image_HszJTN5c4MSPY7KCV4Njwg_0001.png]]
 The percent urban the raster value has to be divided by 9 and then
multiply by 100 to get the actual percent*
*

***** elevation

we use the extract values to point to get the elevation in each
monitoring station*
 [[MIA_PM_10x10_files/img/Image_HDUQwOMnyfepGMJOBrk5aQ_0001.png]]
*

***** traffic density

****** Method 1 (small datasets)

NOTE: this method is for when you have a moderate amount of cases you
want to calculate tden. if the cases are greater then ~ 100,000 use
method 2 (a raster of tden at a 50x50m grid created by steve)
 *Note: make sure that all projections are the same for all layers/shape
files use (we used UTM 19N in NE)*
 we created 50x50 buffers around the pm stations using the buffer from
the toolbox
 NOTE: you have to use the buffer from the tool box to keep the station
id
 [[MIA_PM_10x10_files/img/Image_Fiafp7La2M24Ds82WagjNQ_0001.png]]
 this results in the 50m buffer
 [[MIA_PM_10x10_files/img/Image_Fiafp7La2M24Ds82WagjNQ_0002.png]]
 then intersect these buffers with a all lanes layer in NE layer
 [[MIA_PM_10x10_files/img/Image_Fiafp7La2M24Ds82WagjNQ_0003.png]]
 thus we get all the lanes in the 50x50 buffer
 [[MIA_PM_10x10_files/img/Image_Fiafp7La2M24Ds82WagjNQ_0004.png]]
 we then summarize all length of roads within each buffer by the buffer
id using the summarize option
 [[MIA_PM_10x10_files/img/Image_Fiafp7La2M24Ds82WagjNQ_0005.png]]
 and then choosing the length of roads (sum)
 [[MIA_PM_10x10_files/img/Image_Fiafp7La2M24Ds82WagjNQ_0006.png]]

****** Method 2

NOTE: this method is for when you have a huge amount of cases you want
to calculate tden (if the cases are greater then ~ 100,000)
 We create a raster of line density from the all lanes (A1,A2,A3) across
MIA using the spatial analysit>density>Line density:
 [[MIA_PM_10x10_files/img/Image_MriUSFvqw7uxQY9L62XATA_0001.png]]
 [[MIA_PM_10x10_files/img/Image_MriUSFvqw7uxQY9L62XATA_0002.png]]
 This results in a Raster file 50x50 with the "traffic density" (tden)
 then we use a simple extract to points toolbox to assign each
monitor/case the raster value for tdeb
 [[MIA_PM_10x10_files/img/Image_MriUSFvqw7uxQY9L62XATA_0003.png]]

***** distance to point emission

calculated with spatial join and a distance field to the point emission
layer
 the distance field was multiplied by 100 to get the number in km

***** distance to point A1

calculated with spatial join and a distance field to the A1 roads layer
 the distance field was multiplied by 100 to get the number in km
 finally the combined 50x50 LU files is outputted here:

*c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.1.NE\_PM\_MODELS\3.1.1.4.Work\2.Gather\_data\FN004\_LU\_full\_dataset\lu\_50x50.dbf*

*** 5.Temporal data (Met data)

**** 1.Obtain MET data

data was obtained through the NCDC:

[[http://www7.ncdc.noaa.gov/CDO/cdoselect.cmd?datasetabbv=GSOD&countryabbv=&georegionabbv=][http://www7.ncdc.noaa.gov/CDO/cdoselect.cmd?datasetabbv=GSOD&countryabbv=&georegionabbv=]]
 after choosing the USA i went separately from state to state
 [[MIA_PM_10x10_files/img/SS-2011-03-16_10.47.06.png]]
 then in each *state* i choose ONLY stations that*ONLY*had continous
data for the study period (2000-2010):
 [[MIA_PM_10x10_files/img/SS-2011-03-16_10.47.30.png]]
 then i selected a time rage and *made sure* its exported in csv (,):
 [[MIA_PM_10x10_files/img/SS-2011-03-16_10.50.03.png]]
 finally i download the txt
file[[MIA_PM_10x10_files/img/SS-2010-10-15_11.19.14.jpg]][[MIA_PM_10x10_files/img/SS-2010-10-15_11.19.33.jpg]]

**** 2.process met data

AOD data was imported to a full file covering all years using script
c005\_MET Final
 the met files (both all years and by year) are exported here:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.2.MIA\_PM\_MODELSV2\3.1.1.4.Work\2.Gather\_data\FN005\_MET\_full\_dataset\
 in addition key files (by years since we dont have all stations in
every year) are exported here:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.2.MIA\_PM\_MODELSV2\3.1.1.4.Work\2.Gather\_data\FN004\_Key\_tables\

*** 8.create wieghts and stage2(mod2) files

**** 1.create all aod values for whole year

using script c008\_create\_weights we create first the whole grid for
every day (weather or not it had AOD)
 1) a unique grid file (all grids for one example day) is exported for
mod3 gamm stage:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.2.MIA\_PM\_MODELSV2\3.1.1.4.Work\3.Analysis\AN005\_mod3\uniq\_grid.csv
 2)full all grid files for all days are exported by year for the add
mean pm (MPM) stage:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.2.MIA\_PM\_MODELSV2\3.1.1.4.Work\2.Gather\_data\FN011\_mod3\_pre\_mpm\
 3)weight files per year are created and exported here:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.2.MIA\_PM\_MODELSV2\3.1.1.4.Work\2.Gather\_data\FN009\_Weights\y2000.csv
 4)files for the stage2 part (mod2) are created and are here:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.2.MIA\_PM\_MODELSV2\3.1.1.4.Work\3.Analysis\AN003\_mod2\y2000.csv

*** 12.create MPM for mod3

**** 1.assign mean PM to fullgrid

in GIS we load the unique PM station id (sitecode) from the output of
script cn002 located here:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.2.MIA\_PM\_MODELSV2\3.1.1.4.Work\2.Gather\_data\FN007\_Key\_tables\pm\_sites.dbf
 [[MIA_PM_10x10_files/img/SS-2011-10-27_11.33.48.png]]
 then using spatial join we assign each sitecode the region it falls in
resulting in gn030\_sitecode\_region
 [[MIA_PM_10x10_files/img/SS-2011-10-27_11.37.31.png]]
 this is exported to dbf in the key table folder :

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.2.MIA\_PM\_MODELSV2\3.1.1.4.Work\2.Gather\_data\FN007\_Key\_tables\pmcode\_region.dbf

**** 2.create region XY centroid

in addition for the region file we create a x and y variables manually
(add field) and using calculate geomatry give it a x and y in Wgs 1984
 [[MIA_PM_10x10_files/img/Image_Y0WjgScbeBODTeKhHMgFtA_0001.png]]
 we then export that to dbf:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.2.MIA\_PM\_MODELSV2\3.1.1.4.Work\2.Gather\_data\FN007\_Key\_tables\reg\_centroids\_XY.dbf

**** 2.assign mpm to mod2/mod3

using script c012\_create\_mpm we:
 1)create a mpm (mean PM) file for each year for every day in every
region outputed here:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.2.MIA\_PM\_MODELSV2\3.1.1.4.Work\2.Gather\_data\FN013\_mpm\_peryear\mpm2000.sas7bdat
 *NOTE: before running this script make sure you have already run mod2
so that the mod2 files are available*
 2)assign mpm per region for a day for *mod2 prediction* file and output
here:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.2.MIA\_PM\_MODELSV2\3.1.1.4.Work\3.Analysis\AN004\_mod2pred\T2000\_m2\_pred\_mpm.csv
 3)assign mpm per region for a day for *mod3*prediction file and output
here:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.2.MIA\_PM\_MODELSV2\3.1.1.4.Work\3.Analysis\AN005\_mod3\fullgrid\_mpm\_2000.csv

**** 3.2.assign mpm to mod2/mod3 CV

using script c013\_create\_mpm\_CV\_MACRO we take the created files from
:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.2.MIA\_PM\_MODELSV2\3.1.1.4.Work\2.Gather\_data\FN013\_mpm\_peryear
 and give the 90% CV data files (per year at > AN009\_mod2\_CV\_files)
the mean pm in each region and output it here:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.2.MIA\_PM\_MODELSV2\3.1.1.4.Work\3.Analysis\AN009\_mod2\_CV\_files\_mpm\

*** 13.prepare data for residual\_localpm part

**** 1.create 300 meter buffers from stations

a PM\_Id layer with all pm stations is loaded
 then a 300 m buffer is created around that
 [[MIA_PM_10x10_files/img/SS-2011-08-05_13.16.06.png]]

**** 2.elevation

data for elevation on a 250m resolution was obtained from mi-hye (SEEK
REFERENCE)
 then each 300 meter buffer was calculated the mean elevation:
 [[MIA_PM_10x10_files/img/SS-2011-08-05_13.27.41.png]]
 *
 NOTE: since Each variable is called RASTERVALUE, gis wont be able to
extract values for another variable with the same name (it will give all
values a NULL value). To address this simple create a new variable ,
copy the raster value to it and delete the raster value field
*then the -9999 values are changed to '0'*:
 [[MIA_PM_10x10_files/img/SS-2011-08-05_13.37.27.png]]
*this files is saved as pd\_elevations

**** 3.calculate traffic density

 4. Traffic Density

 - In ArcToolbox, double-click the clip tool and clip the roads by
buffers

 [[MIA_PM_10x10_files/img/Image.jpg]]

 then using the non toolbox spatial join option each buffer of
monitoring stations was given the sum of all cliped roads intersecting
it

 [[MIA_PM_10x10_files/img/SS-2011-08-08_10.38.09.png]]

**** 4.join elev and traffic to buffers

using 2 simple spatial joins by closest the 2 above layers with elev and
total length of roads in each buffer (tden) was joined to the buffer
layers
 results in:
 rn6\_join2\_road

**** 5.land\_cover

**

To calculate land cover (Percentage of urbaness)

 - Add the binary raster of urbaness(refer to Itai's email on criteria
on reclassification of NLCD)

 [[MIA_PM_10x10_files/img/Image_a3HNT75ALE9hgMLbQkyqVQ_0001.jpg]]

 - Run the '*zonal statistics as table*' and select mean (mean for
binary data is percentage)
 NOTE: if the procedure fails try to save it outside of a gdb as a
normal table in a folder

 [[MIA_PM_10x10_files/img/SS-2011-08-08_10.40.40.png]]

 This results in Percentage of open space(technically, percentage of
urbaness)
 [[MIA_PM_10x10_files/img/Image_a3HNT75ALE9hgMLbQkyqVQ_0002.jpg]]

 then using a normal join it was added back to the *rn6\_join2*\_road
layer based on sitecode:
 [[MIA_PM_10x10_files/img/SS-2011-08-08_10.43.57.png]]

**** 6.population density

data was obtained through steve from the lanscan data in raster form
lanscan at a 1x1km resolution
 first using extract to point we extracted the population values to the
monitor points (centroid of the 300m buffers).
 [[MIA_PM_10x10_files/img/SS-2011-08-09_12.04.00.png]]
 this layer rn7 was then joined to the *rn6\_join2*layer and the key
variables only left and renamed in the proccess.
 this results in r*n8
*then we calcualted the pop density by deviding the population by the
area of the buffer (*note: the layer had to be reprojected to utm 18n so
that we could calculate area*)
 *
**
*

**** 7.final data

the final data including all the LU 50x50 terms is located here:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.2.MIA\_PM\_MODELSV2\3.1.1.4.Work\3.Analysis\AN002x\_mod1\_localpm\resid\_lu.csv

*** 15.cluster analysis

**** intro- testing the cluster method

We deciided to test a alternative method to using the bimon way
(essentially using a cluster for every 2 months per year) and to use 9
clusters based on PM levels in each day for the whole period

**** create all year pm dataset for the cluster analysis

using *c015\_prepeare\_pm\_dataset\_forcluster*we prepared a data set
with all years resulting with the *PM* for every*station* in each *day*
 [[MIA_PM_10x10_files/img/SS-2011-11-17_11.14.13.png]]
 in the script stations missing PM for a specific day get the PM from
the next closets (by distance) PM station.
 If a station is still missing it gets the PM from the meanPM file we
create in step 12
 lastly the data is transposed and outputted here:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.2.MIA\_PM\_MODELSV2\3.1.1.4.Work\3.Analysis\AN030\_cluster\pmcluster.csv
 In addition we tested how the cluster preforms every 3 years instead of
1 big yearly file. the big file from above was spllit to 3 3year files
outputted here:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.2.MIA\_PM\_MODELSV2\3.1.1.4.Work\3.Analysis\AN030\_cluster\pmcluster0002.csv

*** 16. PM 10 comparison

**** 1.merge PM data with PM2.5

use script *cn024*

** 3.Analysis
***  Stage 1 (mod1)
**** 1.split files and add weights

using the cn019\_split4cv script in R a weight variable is added to the
aod-pm-lu-met dataset to take into account the larger number of summer
days over winter days
 We incorparate all avilable covariates in the model using a model that
emulates a 'kernel machine'
 each spatial and temporal covariate is transformed to a Z-score by
subtracting the *Mean* from each obs. and deviding by the *SD*:
 example:
 F\_T2001\_All$zelev<-(F\_T2001\_All$CONTOUR-*97.22*)/*165.32*
 and the files are split randomly into the 10% files and 90% files
 this also creates the files needed for the stage2 splits
 *output:*

c:/Users/ekloog/Documents/$Doc/3.PostDoc/3.1.Projetcs/3.1.2.MIA\_PM\_MODELSV2/3.1.1.4.Work/3.Analysis/AN002\_mod1\_CV/out2000.dbf
 *and for mod 2:
*

c:/Users/ekloog/Documents/$Doc/3.PostDoc/3.1.Projetcs/3.1.2.MIA\_PM\_MODELSV2/3.1.1.4.Work/3.Analysis/AN003\_mod2/sas/out2001.dbf

**** 2.run mod1 for full dataset

Using sas scripts*c020\_mod1\_100percent* a mixed model is run in SAS to
calibrate the ground station PM with AOD including spatial and temporal
variables. the outputs of the model is here:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.2.MIA\_PM\_MODELSV2\3.1.1.4.Work\3.Analysis\AN002\_mod1\_CV\overall\_random\
 *NOTE:*when the covariance paramters are not '0' we get the estimation
for all the random effects. Since the model is very complex we are
missing some covariance parameters for the day-region combination (for
example the random slope for Temp per region)
 therefore we use the output of the 100% dataset (full) and take the
covariance parameter estimates to be used in the*CV iterations model*
 this could happen in the 100% dataset as well so you use the same
method in the model there.
 [[MIA_PM_10x10_files/img/SS-2011-10-26_10.13.58.png]]
 [[MIA_PM_10x10_files/img/SS-2011-10-26_10.31.18.png]]

**** 3.Run first mixed models and CV (mod1)

after adding the boundaries from the full model (prev step) to the
individual sas scripts*c020macro\_mod1CV\_2000 to
c020macro\_mod1CV\_2008*we use a batch file *c020macro\_mod1CV\_BATCH*
to run CV for all years.
 The CV mixed models predict for the 10% files from the 90% files
 the files for each year are aggregated to one large file and outputted
here:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.2.MIA\_PM\_MODELSV2\3.1.1.4.Work\3.Analysis\AN002\_mod1\_CV\sas
export\t2008\_all\_10p.csv
 *NOTE: we also used script* *c020\_mod1\_CV\_R
*c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.2.MIA\_PM\_MODELSV2\3.1.1.3.Code\Archive\$brents
R method for CV\c020\_mod1\_CV\_R.R to compare the SAS results with R and got the same results

**** 4.add local PM stage for CV results

in addition we add the local pm (resid) part to this script
*c020\_mod1\_100percent\_localPM
*

*** 2. Stage 2

**** 1.Run second mixed model (mod2)

using *c021\_mod2* a macro is run and the fit from the full step 1 model
(mod1) is used to predict PM for the mod2 files (ALL avilable AOD
points)
 the files with predictions are located here:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.2.MIA\_PM\_MODELSV2\3.1.1.4.Work\3.Analysis\AN004\_mod2pred\T2000\_m2\_pred.dbf
 after this *run the MPM stage to get mpm for all these predictions*

**** 2.run second stage cross validation

using script*c032\_mod2\_CV\_MACRO.sas* the CV files for mod2 are
prepared

*** 3. Stage 3

**** 5.Run GAMM stage model (mod3)

using *c023\_gamm* we run the GAMM stage. we then predict for the all
points (all days in all the grid).
 the files are split into the prediction files from the lme outputted
here as dbf:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.2.MIA\_PM\_MODELSV2\3.1.1.4.Work\3.Analysis\AN006\_mod3pred\grid\_2000\_bimon1.dbf
 and the prediction part from the smoothing outputted as csv here:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.2.MIA\_PM\_MODELSV2\3.1.1.4.Work\3.Analysis\AN006\_mod3pred\T2000\_bimon1.csv

**** 5.Run CV GAMM (mod3 CV)

using *c032\_gamm\_all\_CV* we run the GAMM CV stage
 the outputs are saved in the results file for all years here:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.1.NE\_PM\_MODELS\3.1.1.5.Results\mod3cv\mod3CV\_r2.csv

*** 4.create full poll dataset

**** 1.merge all predcitions

using script *c039\_create\_poll\_datasets.sas*we :
 2) combine all datasets into 1 (by guid and date)
 3)choose the PM predicted value (pmnew) according to the best avilable
 NOTE: though initially we used actual PM for best then mod2 and finally
mod3, in the Final dataset we just used mod2+mod3 since actual pm from
stations could be biased from lets say a major highway close by to a
station and thus we decided to use mod2+mod3 that cover a 10x10 grid and
not just a measurement location
 this was done with a "if then" statements (see script).
 this resulted in a data sets for pm2.5 predictions incorporating ALL
models for New-England for 2000-2008.

**** 2.create lag files

using script *cn60\_add\_Temp\_and\_lags*
 we import the individual files, add them to a full GRID (so that
missing days in the very rurual areas get a missing values )
 then run the lags

*** cluster analysis

**** 1.create clusters for all years

using *c060\_clusters* we create the clusters for the cluster analysis
 we use the kmeans (ssi criteriion) and look at the output (see *red dot
in red box*). this shows in the example we need 5 clusters
 [[MIA_PM_10x10_files/img/SS-2011-11-17_12.27.45.png]]
 we then extract the cluster data base to a file
 NOTE: all clusters are present in the file and you have to choose the
correct cluster variable in your analysis. in the above example its the
*5groups* (we had 5 clusters):
 [[MIA_PM_10x10_files/img/SS-2011-11-17_12.32.58.png]]

**** 2.create clusters for individual years

using *c060\_clusters* we also create the clusters for individual years
in the 2nd part of the script
 we use the kmeans (ssi criteriion) and look at the output (see *red dot
in red box*). this shows in the example we need 3 clusters for the 3
year period
 [[MIA_PM_10x10_files/img/SS-2011-11-17_13.18.40.png]]
 we then extract the cluster data base to a file
 NOTE: all clusters are present in the file and you have to choose the
correct cluster variable in your analysis. in the above example its the
*3groups* (we had 3 clusters):
 [[MIA_PM_10x10_files/img/Image_C5RxCbmsyYQWZgFIUI6gdQ_0001.png]]

*** 5.LPM map

**** 1.predict from pm stgations to full grid

using this script cn070\_LPM\_200x200 we calculate the local pm based on
the fit of the pm stations (resid~LU terms)
 this is exported here:

*c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.2.MIA\_PM\_MODELSV3\3.1.1.4.Work\3.Analysis\AN070\_LPM\_stage\_200x200\_base\lu200LPM.dbf*

**** 2.create raster in GIS

in GIS the DBF file of all LPM points and resid predictions is loaded
and using the
 [[MIA_PM_10x10_files/img/Image_U9VdxiqvpVxcUUWkBXKCAQ_0001.png]]
 we convert the files into a rater to present as a map
*** 6.calculate RMPSE
we used this script

f:\Uni\Projects\3.1.2.MIA_PM_MODELSV3\3.1.1.3.Code\3.Analysis\cn80_calculate_RMSE_CV_mod1_spatial.sas

#+BEGIN_SRC sas

options mprint;
%macro import(year=);


/*erros will occure in teh import due to sitecode charatcer/numeirc issues..ignore that*/

PROC IMPORT OUT= Pdataa_&year
  DATAFILE= "c:/Users/ekloog/Documents/$Doc/3.PostDoc/3.1.Projetcs/3.1.2.MIA_PM_MODELSV3/3.1.1.4.Work/3.Analysis/AN080_files_4CV_RMSE/t&year..csv"
    DBMS=CSV REPLACE;
	  GETNAMES=YES;
	    DATAROW=2;
		RUN;




proc summary nway data=Pdataa_&year;
class sitecode;
var pm25 OApred;
output out=Pdataa_&year.agg mean=barpm barpred;
run;

proc sort data = Pdataa_&year ;by sitecode   ;run;
proc sort data = Pdataa_&year.agg ; by sitecode ;run;

data Pdataa_&year;
merge Pdataa_&year(in=a) Pdataa_&year.agg (in=b)  ;
  by sitecode;
    if a;
	run;



/* summarize the results of the cross-validations */
data sumPdataa_&year;
  set Pdataa_&year;
  d = barpm-barpred;
  absd=abs(d);
run;

/*

Mean absolute error (MAE)

The MAE measures the average magnitude of the errors in a set of forecasts,
without considering their direction. It measures accuracy for continuous variables.
The equation is given in the library references.
Expressed in words, the MAE is the average over the verification sample of the absolute
values of the differences between forecast and the corresponding observation.
The MAE is a linear score which means that all the individual differences are weighted equally in the average.

Root mean squared error (RMSE)

The RMSE is a quadratic scoring rule which measures the average magnitude of the error.
The equation for the RMSE is given in both of the references.
Expressing the formula in words, the difference between forecast and
corresponding observed values are each squared and then averaged over the sample.
Finally, the square root of the average is taken. Since the errors are squared before
they are averaged, the RMSE gives a relatively high weight to large errors.
This means the RMSE is most useful when large errors are particularly undesirable.

*/


proc summary data = sumPdataa_&year;
  var d absd;
  output out= Error_&year std(d)=rmse1 mean(d)= mpe1 mean(absd)= mae1;
run;

data Error_&year(drop = _Type_ _FREQ_);
 set Error_&year;
  Year = &Year;
run;

data Error_&year(drop = rmse1 mpe1 mae1);
 set Error_&year;
  rmse  = round(rmse1,0.001);
  mpe   = round(mpe1, 0.0000000000001);
  mae   = round(mae1, 0.001);
run;

%MEND ;

%import(year=2000);

#+END_SRC

to calculate the RMPSE .We calculated prediction errors for the *spatial components* in each stage (to be comparable to all previous available model which don’t have daily measurements) by subtracting retained observations from the model predictions. We estimated the model prediction precision by taking the square root of the mean squared prediction errors (RMSPE) for each year
