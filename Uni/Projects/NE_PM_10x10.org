#+TITLE: NE_PM_10x10

* 3.1.1-NE\_AOD

** 1.Intro

*** Script RUN order

!$Note-we didnt use Regions in the final NE model, take $Note!

Index file for process order of PM predictions from AOD files
$NOTE: there is a separate set of scripts for some steps for the2009-2011 data since the AOD,PM and TEMP data sources changesd a bit, the new scripts come after the\ sign$


1) c001 AOD Final.sas/c051 import AOD NEW
2) c002 PM Final.sas/c050 import EPA PM
Note:for the years 2008 onwards the PM is hourly/4 hours and NOT 24
hour means and thus we add another step to aggreagte up to daily means
NOTE2: midway in the script (after you create the pmsite file) you now
need to create The regions in GIS for the study area and create a
keytable for sitecode and guid (see section 7> 1-3 )

3) c003 J AODPM Batch.sas

4) c005 MET Final.sas
-manually transfer the all world NCDC met file (metXY.dbf) to the
location:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.1.NE PM MODELS\3.1.1.1.Raw data\NCDC
Met data\metXY.dbf
before starting to run the script.
 -Change the state ABriv letters to corresopond to study area

5) proceed to build in GIS all LU/emission dataset (see section 4)

6) c006 J AODPM METLU Batch.sas
this step is long, if possible leave it overnight
Check in each year that the sitecode isn't truncated

7) c008 create weights.sas
note: check there are no missing values in the weight and mod 2 files!!
this can cause to missing grids
8)c010 add extra lu.sas
*note:
-this was done since tden and forestry was not added originally (not
needed if you add them to the main LU emission file from onset.
-after this manually replace the t2000 and y2000 in mod1 and mod2 with
the old one*
Analysis

8)c019 split4cv.r/c019 split4 09124cv

9) c020 mod1 100percent.sas
-make sure to save the mixed model outputs for the parameter
constraints

10) c020 mod1 CV BATCH.sas

11) c020 mod1x 10percent localPM

12) c021 mod2 100percent.sas

13) c022 mod2 CV MACRO.sas

14) c012 create mpm noRegion
-note: if the number or regions are not enough in a a specific year
(see output for missing) you need yo use proc expand, see 2005 in NE as
an example
-in 2005 there is no data for Jan 18th, so we manually calculate the
mpm from prev/next day...see code for details

15) c013 create mpm CV MACRO

16) c014 prepeare pm dataset forcluster.sas
-note this is only needed if you subset the data by population

17) c023 gamm 100percent.r

18) c024 mod3 100percent R2

19) c033 gamm CV.r

20) c034 mod3 CV R2.sas

21) c035 mod3 CV localPM

19) c039 create poll datasets.sas

20) cn60 add Temp and lags

*** model iterations

the current model predicts only for grids with a population density <10

*** 1.modis info

the way to extract the data if not from bill ridgway is:
 get the 10x10km grid from modis
 then in each grid cell you calculate the average PM2.5 falling that day
in the box
 There are two MODIS Aerosol data product files: *MOD04 L2*, containing
data collected from the Terra platform; and *MYD04 L2*, containing data
collected from the Aqua platform.
 Pasted from
<[[http://modis-atmos.gsfc.nasa.gov/MOD04_L2/index.html][http://modis-atmos.gsfc.nasa.gov/MOD04 L2/index.html]]>

*** hyung-joo LEE manual modis extraction

Since in petroses' group they didnt have ready made data they had to
extract the MODIS AOD data manually:
 1)they downloaded daily HDF files for the study area
 2)they created a grid manually (10x10km) over the studay area. the
start position of the 10x10 grid was arbitrary
 3)using an IDL script they developed and that they ran seperatly for
each day they extracted the aod values , x cord , y cord and other
related variables (cloud coverage, solar zenith) that might Bias AOD
reading
 4) this script resulted in a DB file (in excel) which was then
calibrated with ground stations as we did

** 2.Gather data

*** 1.create daily aod data

**** 1.obtain data

AOD data was obtained through modis satellites with the help of Bill
Ridgway (Bill.Ridgway@nasa.gov).
 The files downloaded were monthly txt files with XY data and
corresponding AOD data for each pixel. Each pixel has a resolution of
10x10km.
 There are two MODIS Aerosol data product files: *MOD04 L2*, containing
data collected from the Terra platform; and *MYD04 L2*, containing data
collected from the Aqua platform.
 Pasted from
<[[http://modis-atmos.gsfc.nasa.gov/MOD04_L2/index.html][http://modis-atmos.gsfc.nasa.gov/MOD04 L2/index.html]]>

**** 2.process data

AOD data was imported to a full file covering all years using script
c001 AOD Final.sas
 the output is here:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.2.NEW\3.1.1.4.Work\2.Gather data\FN001 AOD full dataset\all aod.sas7bdat
 in addition and AOD grid (unique points with no dates) is created and
outputed here:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.1.NE PM MODELS\3.1.1.4.Work\2.Gather data\FN004 Key tables\aodgridfull.dbf

*** 2.daily pm
**** 1.download PM data from Views website

*NOTE: use windowz explorer!!*
 yearly pm data was downloaded from the 'views' website
 *1) hold down control and drag the mouse to multi select the individual
stations*

Dataset,SiteCode,Date,POC,SiteName,Latitude,Longitude,State,CountyFIPS,CountyName,EPACode,MF:Value
 [[NE_PM_10x10_files/img/SS-2011-03-16_08.40.36.png]]

**** 2.create data
AOD data was imported to a full file covering all years using script
c002 PM Final.sas
 PM single year dataset are exported here:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.1.NE PM MODELS\3.1.1.4.Work\2.Gather data\FN002 PM full dataset\
 A single Pm station grid is also created (pm sites) and exported to
dbf:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.1.NE PM MODELS\3.1.1.4.Work\2.Gather data\FN004 Key tables\pm sites.dbf
 datasets for the mod3 predictions are created when the single pm
stations are joined with a keytable of guid sitecode and outputed here:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.1.NE PM MODELS\3.1.1.4.Work\3.Analysis\AN008 mod3 corr\pmguidt&year.dbf

*** 3.create aodpm

**** 1.Create the AOD-PM joined dataset

using script c003 J AODPM Final we joined the AOD dataset and PM
dataset by distance, that is the closet AOD point to each PM station at
a given buffer (13km).
 data is outputed here:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.1.NE PM MODELS\3.1.1.4.Work\2.Gather data\FN003 PM AOD Combined\t2007.sas7bdat
 *NOTE: there is a script* c003 J AODPM Final all *that creates these
joins for all years but its very resource intensive and thus its in the
archive folder and you need to run the normal script year by year.*

*** 4.Spatial data (Land use-emissions)

**** 1.10x10km LU

***** 0.Land use data Sources

[[NE_PM_10x10_files/attach/midatlanticvariables.doc][Attachment #01
(midatlanticvariables.doc)]]
 LAND use data was obtained through steve melly from the following
sources (see attachment for detailed disicription):
 Elevation: data was obtained in the form of a raster image from the
USGS DEM
 population/pop density: obtained from the ESRI census datasets
 % open space/urban: obtained through the National land cover raster
 road dist: from the ESRI streetmap layes
 EPA point and area emission: through the EPA website
 *NOTE: attached is a document with some variable info*

***** 1.clip data to only cover the study area

In GIS the aodgrid dbf from the aod creation step

*c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.1.NE PM MODELS\3.1.1.4.Work\2.Gather data\FN004 Key tables\aodgridfull.dbf*
 was imported to GIS and a polygon layer of the study area was added
 [[NE_PM_10x10_files/img/SS-2011-04-01_10.33.27.png]]
 using clip, only the relevant AOD points falling in the study area were
left (gn1)
 [[NE_PM_10x10_files/img/SS-2011-03-17_13.09.12.png]]
 [[NE_PM_10x10_files/img/SS-2011-04-01_10.36.45.png]]
 Then the cliped data was spatially joined to the region layer so that
each guid got the region it belonged to (*gn11*)
  

***** 2.Dist to road

the guid layer (gn1) was loaded and the A1 layer raw data was loaded
 [[NE_PM_10x10_files/img/SS-2011-04-07_11.44.53.png]]
 using spatial join each guid point got the distance from the closet
road (*gn lu s1*)
 this was then multipled by 100 to get it in km
 [[NE_PM_10x10_files/img/SS-2011-04-07_11.43.12.png]]

***** 3.Elevation

a raster image from NED (through steve) was loaded to GIS
 [[NE_PM_10x10_files/img/SS-2011-04-07_11.47.00.png]]
 then using the extract values (the hight is the 'value' field) function
every guid point go an intepulated elevation value from the closet
elevation points around it:
 [[NE_PM_10x10_files/img/SS-2011-04-07_12.31.45.png]]
 this resulted in elevation values for each guid (*gn lu s2*)
 **Note**- a few points got -9999 so using the geo calculator i assgined
the value 0 (they are all on the coast see map below)
 [[NE_PM_10x10_files/img/SS-2011-04-07_12.31.54.png]]

***** 4.population

population data was obtained through steve from the esri tract data.
 The 10x10 polygon grid created [[#gBZuEmyysIdx5pHEVqXmeQ][HERE]] was
exported to a polygon feature class in the UTM Zone 19 N projection.
These polygons were *intersected* with the 2000 census blocks for New
England also in the UTM Zone 19 projection.
 Microsoft Access was used to calculate estimated population and
population density in 10 km grid cells.
 The table resulting from the intersection of blocks with grid cells was
exported to MS Access. Tables with the census block area and 10 km grid
areas were also exported. The nominal 10 km grid cells do not have
identical areas since they were created in the WGS 1984 geographic
coordinate system, and areas were calculated using the UTM Zone 19
projection.
 MS Access queries were used to:
 *calculate the fraction of each census block in each grid cell:*
 Blkfrac = gridblkarea/blkarea
 *Estimate the population in each grid-block intersection:*
 Gridblkpopest = blkfrac * totpop (if blkfrac > 1 due to rounding errors
then gridblkpopest = totpop)
 *Sum up the estimated populations for the 10 km grid cell and divide by
grid areas:*
 Gridpopest = ∑ gridblkpopest
 Gridpopdens = gridpopest / gridarea
 the shape file is located here:

*C:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.1.NE PM MODELS\3.1.1.4.Work\2.Gather data\AGIS DB\NE AOD.gdb\grid cliped LU
*

***** 5.Open space

Data was obtained through the national land cover database and proccesed
by steve
 See [[#GxTXQkGqH9FQ6v5xqeGSAA][percent of forestry]] to see how the
%open space was calculated
 then using spatial join the *gn lu s3*layer was joined to this layer
producing *gn lu s4*.

***** 6.emissions

****** 1.download from EPA site

data was downloaded form the EPA emissions site:

[[http://www.epa.gov/ttn/chief/net/2005inventory.html#inventorydata][http://www.epa.gov/ttn/chief/net/2005inventory.html#inventorydata]]
 2 files were downloaded : the point emissions and area emissions
 [[NE_PM_10x10_files/img/SS-2011-03-18_09.44.38.png]]
 then the mdb files were open in Access and only the relevant
counties/states were slected
 in this subset only the PM annual emission was selected (pollutant  =
'PM25-FIL' OR pollutant  = 'PM25-PRI' OR )
 [[NE_PM_10x10_files/img/SS-2011-03-18_09.52.35.png]]

****** 2.Area Emmisons

the Area dataset was loaded into GIS as a table and using regular join
*added to a empty NE Tracts polygon file based on the FIPS ID.
 [[NE_PM_10x10_files/img/SS-2011-04-08_10.33.06.png]]*
 the resulting was the area emissions in each county/tract
 then the *gn lu s4* file was spattialy joined to the area emmison
file so that each guid got the area emission in was closest to/inside it
 resulting in *gn lu s5.*

****** 3.Point Emmisons

in GIS the emissions points were added to the latest working layer
(*gn lu s5*):
 1)first the 2 point emission data sets were joined (the point xy file
and pm data)
 [[NE_PM_10x10_files/img/SS-2011-04-08_10.38.28.png]]
[[NE_PM_10x10_files/img/SS-2011-04-08_10.38.46.png]]
 this point data was then joined to a 10x10km grid polygon layer . the
'Average' option was used so each polygon got the average emssions (tons
per year) *gn lu s6*.
 Polygons with 0 emissions go the values of half the lowset station
emittion (0.001). this was done by selecting all polygons with
 '0' values and with the field calculator assigning a 0.001 value
 [[NE_PM_10x10_files/img/SS-2011-04-08_12.44.54.png]]
 then this poly file was joing to *gn lu s5*to get the final dataset
with all LU-emission data *gn lu s7.*
 this file is *exported to a dbf:*

*c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projet3.1.1.NE PM MODELSSV2\3.1.1.4.Work\2.Gather data\FN006 LU full dataset\lu emission.dbf
*

***** 7.traffic density

we also decided to add traffic density top the model where traffiic
density (tden) was defined as the total length of lanes (from all roads
a1,a2,a3) in the 10x10 grid cells.
 this was simply calculated with a spatial join where the *sum*of all
lanes was calculated for each 10x10 polygon
 [[NE_PM_10x10_files/img/Image.png]]
[[NE_PM_10x10_files/img/Image_6wvLoz9DByMLlSH5SEGKbg_0001.png]]
 the *results/output* are located here in :

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.1.NE PM MODELS\3.1.1.4.Work\2.Gather data\FN004 LU full dataset\tden.dbf
  

***** 8.percent forestry

[[NE_PM_10x10_files/attach/c016_calculate%20forestry.sas][Attachment #02
(c016 calculate forestry.sas)]]
 a national land cover database was proccesed and reclassified to 0,1
(forest areas or not-by ncdc code)
 the 10x10 polygon layer was loaded and a grid2 field (text) was created
for the zonal statistic part
 [[NE_PM_10x10_files/img/Image_GxTXQkGqH9FQ6v5xqeGSAA_0001.png]]
 then using *zonal statistics as table* the mean ratio between 0(no
forest) and 1 (forested) was calculated (using the guid2 text variable
as zone field) and outputed as a table:
 [[NE_PM_10x10_files/img/Image_GxTXQkGqH9FQ6v5xqeGSAA_0002.png]]
 then the mean value was multiplied by 100 to get the % forested
 [[NE_PM_10x10_files/img/Image_GxTXQkGqH9FQ6v5xqeGSAA_0003.png]]
 Finally the percent forsted was joined (by table) to the 10x10 layer

****** old method

NOTE-THE OLD METHOD WORKS BUT IT CORRUPTS THE NUMERICAL GUID WHEN YOU
USE COMBINE AND THUS USE THE ZONAL STATISTICS METHOD
 a national land cover database was proccesed and reclassified to 0,1
(forest areas or not-by ncdc code)
 then a 10x10 guid point grid was transformed into a raster
 *Note: the raster cell size needs to match grid resolution you want.
below its in degrees so its 0.1 degree (10km), also the value field
needs to be the ID field of the grids*
 [[NE_PM_10x10_files/img/SS-2012-04-23_15.45.25.png]]
 then we combine the 2 raster (the 10x10 raster and forest raster).
 *make sure that in the environment you set the cell size to be the one
of the smallest of the 2 rasters (the forest one)*
 [[NE_PM_10x10_files/img/SS-2012-04-23_15.53.20.png]]
 the result raster has a count field for each forest type (0,1) for each
grid id
 [[NE_PM_10x10_files/img/Image_J8XgOZTyJ1P6oxsHtWr7kw_0001.png]]
 this table is exported to DBF and then using SAS code c016 calculate
forestry the percent was calculated, see code for more detail (attached)

**** 2.50x50 local LU

***** 0.intro

NOTE:make sure all layers are projected in UTM before starting to avoid
issues
 I have an idea for our Massachusetts data. We can make use of the exact
geocodes by doing the following. Take the residuals from our model for
the grid cells with monitors. Regress them against land use terms local
to the monitor (distance to road, pop density within 300 m, land use
within 300 m). Hopefully, this will explain something, and we can then
get those terms for our Mass mortality and births, but also the NAS,
Framingham and VIVA cohorts, and get better predictions.
 All LU data was obtained through steve (see 2.1)
 all outputs are located here:

*c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.1.NE PM MODELS\3.1.1.4.Work\2.Gather data\AGIS DB\NE LU50x50.gdb*

***** population density

Population density was added to the point layer of monitoring stations:
 1)first we used the extract values to points. the raster values were
already the population density
 [[NE_PM_10x10_files/img/SS-2011-06-22_10.13.09.png]]
 [[NE_PM_10x10_files/img/SS-2011-06-22_10.16.05.png]]
 *
*

***** percent urban

****** Raw data and calculation

This shows how steve calculated the percent urban variable
 1) first a source raster image was downlaoded from the national land
cover dataset (through steve) at a resolution of 30x30 meters.
 This raster was loaded to GIS
 *
*2)**then using the reclassify tool in spatial analyst we recallified
the raster into:
 0-open space
 1-urban areas
 using the available raster values (22,23,24 were the corresponding NLCD
values for urban areas)
 this was exported to a *new raster*
 *[[NE_PM_10x10_files/img/Image_gSme8PmOBx6xPoPIinQwYg_0001.png]]*
 3) then focal statistics were used. Any cell center encompassed by the
circle will be included in the processing of the neighborhood
 [[NE_PM_10x10_files/img/Image_gSme8PmOBx6xPoPIinQwYg_0002.png]]
 in statistics type we choose sum to get all cells with the urban value
in them
 we choose the 3, HxW to get the surrounding 9 cells.
 4)that raster obtained:
 [[NE_PM_10x10_files/img/Image_gSme8PmOBx6xPoPIinQwYg_0003.png]]
 had the value of the sum of all urban areas (the '1' values) in each 30
meter cell we defined before

****** Add to LU data

*NOTE: since Each variable is called RASTERVALUE, gis wont be able to
extract values for another variable with the same name (it will give all
values a NULL value). To address this simple create a new variable ,
copy the raster value to it and delete the raster value field
 [[NE_PM_10x10_files/img/SS-2011-06-22_12.56.54.png]]
*
 We use extract value to points to get the raster value in each 50 meter
grid cell
 [[NE_PM_10x10_files/img/Image_HszJTN5c4MSPY7KCV4Njwg_0001.png]]
 The percent urban the raster value has to be divided by 9 and then
multiply by 100 to get the actual percent*
*

***** elevation

we use the extract values to point to get the elevation in each
monitoring station*
 [[NE_PM_10x10_files/img/Image_HDUQwOMnyfepGMJOBrk5aQ_0001.png]]
*

***** traffic density

****** Method 1 (small datasets)

NOTE: this method is for when you have a moderate amount of cases you
want to calculate tden. if the cases are greater then ~ 100,000 use
method 2 (a raster of tden at a 50x50m grid created by steve)
 *Note: make sure that all projections are the same for all layers/shape
files use (we used UTM 19N in NE)*
 we created 50x50 buffers around the pm stations using the buffer from
the toolbox
 NOTE: you have to use the buffer from the tool box to keep the station
id
 [[NE_PM_10x10_files/img/Image_Fiafp7La2M24Ds82WagjNQ_0001.png]]
 this results in the 50m buffer
 [[NE_PM_10x10_files/img/Image_Fiafp7La2M24Ds82WagjNQ_0002.png]]
 then intersect these buffers with a all lanes layer in NE layer
 [[NE_PM_10x10_files/img/Image_Fiafp7La2M24Ds82WagjNQ_0003.png]]
 thus we get all the lanes in the 50x50 buffer
 [[NE_PM_10x10_files/img/Image_Fiafp7La2M24Ds82WagjNQ_0004.png]]
 we then summarize all length of roads within each buffer by the buffer
id using the summarize option
 [[NE_PM_10x10_files/img/Image_Fiafp7La2M24Ds82WagjNQ_0005.png]]
 and then choosing the length of roads (sum)
 [[NE_PM_10x10_files/img/Image_Fiafp7La2M24Ds82WagjNQ_0006.png]]

****** Method 2

NOTE: this method is for when you have a huge amount of cases you want
to calculate tden (if the cases are greater then ~ 100,000)
 We create a raster of line density from the all lanes (A1,A2,A3) across
NE using the spatial analysit>density>Line density:
 [[NE_PM_10x10_files/img/Image_MriUSFvqw7uxQY9L62XATA_0001.png]]
 [[NE_PM_10x10_files/img/Image_MriUSFvqw7uxQY9L62XATA_0002.png]]
 This results in a Raster file 50x50 with the "traffic density" (tden)
 then we use a simple extract to points toolbox to assign each
monitor/case the raster value for tdeb
 [[NE_PM_10x10_files/img/Image_MriUSFvqw7uxQY9L62XATA_0003.png]]

***** distance to point emission

calculated with spatial join and a distance field to the point emission
layer
 the distance field was multiplied by 100 to get the number in km

***** distance to point A1

calculated with spatial join and a distance field to the A1 roads layer
 the distance field was multiplied by 100 to get the number in km
 finally the combined 50x50 LU files is outputted here:

*c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.1.NE PM MODELS\3.1.1.4.Work\2.Gather data\FN004 LU full dataset\lu 50x50.dbf*

**** 3.200x200 local LU

***** intro

NOTE:make sure all layers are projected in UTM before starting to avoid
issues

***** tden

traffic density was created by
 1)creating a raster of all roads across the MIA with the density
function using the spatial analysit>density>Line density:
 [[NE_PM_10x10_files/img/Image_os0nZ0JkQhayE7LcD3fJiA_0001.png]]
 [[NE_PM_10x10_files/img/Image_os0nZ0JkQhayE7LcD3fJiA_0002.png]]
 This results in a Raster file 50x50 with the "traffic density" (tden)
 then we use a simple extract to points toolbox to assign each
monitor/case the raster value for tdeb
 [[NE_PM_10x10_files/img/Image_os0nZ0JkQhayE7LcD3fJiA_0003.png]]

***** dist to A1 and point emission

NOTE2: make sure in all future calculation to be consistent and cut all
above 40km variables to 40!!!
 For these calculations since the dataset was HUGE (~24,000,000) we used
the near feature
 *The values are later divided by 1000 to get the distance in Km and not
meter*
 [[NE_PM_10x10_files/img/SS-2012-06-04_09.53.19.png]]
 NOTE:since the Dataset was HUGE we limited the search for only 40km

***** popden,tden,purban

all these were calculated by steve and are here as a sas file

**** pbl data 
obtained from here:

http://www.esrl.noaa.gov/psd/data/gridded/data.narr.html

proccesed with IDLs
*** 5.Temporal data (Met data)

**** 1.Obtain MET data

data was obtained through the NCDC:

[[http://www7.ncdc.noaa.gov/CDO/cdoselect.cmd?datasetabbv=GSOD&countryabbv=&georegionabbv=][http://www7.ncdc.noaa.gov/CDO/cdoselect.cmd?datasetabbv=GSOD&countryabbv=&georegionabbv=]]
 after choosing the USA i went separately from state to state
 [[NE_PM_10x10_files/img/SS-2011-03-16_10.47.06.png]]
 then in each *state* i choose ONLY stations that*ONLY*had continous
data for the study period (2000-2010):
 [[NE_PM_10x10_files/img/SS-2011-03-16_10.47.30.png]]
 then i selected a time rage and *made sure* its exported in csv (,):
 [[NE_PM_10x10_files/img/SS-2011-03-16_10.50.03.png]]
 finally i download the txt
file[[NE_PM_10x10_files/img/SS-2010-10-15_11.19.14.jpg]][[NE_PM_10x10_files/img/SS-2010-10-15_11.19.33.jpg]]

**** 2.process met data

AOD data was imported to a full file covering all years using script
c005 MET Final
 the met files (both all years and by year) are exported here:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.1.NE PM MODELS\3.1.1.4.Work\2.Gather data\FN005 MET full dataset\
 in addition key files (by years since we dont have all stations in
every year) are exported here:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.1.NE PM MODELS\3.1.1.4.Work\2.Gather data\FN004 Key tables\

*** 6.Join AOD-PM-MET-LU data

**** 1.create a full met dataset and join to aodpm and LU data

script c006 J AODPM METLU we create a full met dataset for each year
and include next closest stations for days that have missing data in
some days
 then the closest met station to each PM station was calculated
 LU data from the spatial data step is joined
 Also in this script we create a key table for guid and stn stations
 *Outputs:*
 1) a complete yearly met file (all STN with full 365 data where missing
data for STN was brought in from closest avilable STN) is here:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.1.NE PM MODELS\3.1.1.4.Work\2.Gather data\FN005 MET full dataset\
 2)the complete AODPM MET LU file for Mod1 in SAS output file is here:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.1.NE PM MODELS\3.1.1.4.Work\2.Gather data\FN006 J AODPM METLU\mod1 2000 prew.sas7bdat
 While the file for R is outputed here:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.1.NE PM MODELS\3.1.1.4.Work\3.Analysis\AN001 mod1\t2000.csv
 3)the key table for guid and stn stations which is outputed here:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.1.NE PM MODELS\3.1.1.4.Work\2.Gather data\FN007 Key tables\guid stn 2000.sas7bdat

*** 7.key tables

**** 1.create regions

using the state borders we merged all into 1 polygon and then devied the
area into 3: north , central ,south
 the file is save din the arcgis DB in:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.1.NE PM MODELS\3.1.1.4.Work\2.Gather data\AGIS DB\NE AOD.gdb
 under regions final
 [[NE_PM_10x10_files/img/Image_Y20FpEyKSTS739xAJIIedQ_0001.png]]

**** 2.create cliped grid and assign region

[[NE_PM_10x10_files/img/Image_mSlDylrObpbGrUM9u4fmBQ_0001.png]]
 using the clip function all aod points in the area were clipped to the
NE land area (*grid clipped*)
 then we join that to the region layer to get the region each guid
belongs to
 [[NE_PM_10x10_files/img/Image_mSlDylrObpbGrUM9u4fmBQ_0002.png]]
 resulting in *grid clipped regions
*export this here for later use in script
c006 J AODPM METLU Batch.sas:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.1.NE PM MODELS\3.1.1.4.Work\2.Gather data\FN007 Key tables\guid region.dbf

**** 3.create sitecode guid keytable

using the clipped grid with regions (*grid clipped regions*) and the
PM station sitcode dbf output
(c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.1.NE PM MODELS\3.1.1.4.Work\2.Gather data\FN007 Key tables\pm sites.dbf)
we create a sitecode guid dataset using spatial join
 the output is here:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.1.NE PM MODELS\3.1.1.4.Work\2.Gather data\FN007 Key tables\guid sitecode.dbf

**** 4.all grids with a sitecode within 9km

using a simple empty grid we do a spatial join to all sitecode layer to
get a list of all gridpoint with grid point close within 9k
 NOTE: grid point with no close stations will be NULL
 later i manually select only grid codes with sitecodes and export that
to dbf at:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.1.NE PM MODELS\3.1.1.4.Work\2.Gather data\FN007 Key tables\guid sitecode within9km.dbf

**** 5.create AOD polygon 10x10 layer

Grid points in WGS 1984 spaced 0.1 degree apart were used to create a
polygon feature class covering New England.
 The Create Fishnet tool was used with the parameters shown below.
 [[NE_PM_10x10_files/img/SS-2012-03-15_11.31.27.png]]
 The Extent *was modified manually* to extend*0.05 degrees past the grid
points*, so that the grid points would be the centroids of the newly
created grid polygons.

A spatial join using the intersect option was used to add the grid id
 (guid) to the polygon grid cells.

The result of the Create Fishnet tool had unspecified spatial
reference.  In ArcCatalog the spatial reference was manually set to WGS
1984.

*** 8.create wieghts and stage2(mod2) files

**** 1.create all aod values for whole year

using script c008 create weights we create first the whole grid for
every day (weather or not it had AOD)
 1) a unique grid file (all grids for one example day) is exported for
mod3 gamm stage:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.1.NE PM MODELS\3.1.1.4.Work\3.Analysis\AN005 mod3\uniq grid.csv
 2)full all grid files for all days are exported by year for the add
mean pm (MPM) stage:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.1.NE PM MODELS\3.1.1.4.Work\2.Gather data\FN011 mod3 pre mpm\
 3)weight files per year are created and exported here:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.1.NE PM MODELS\3.1.1.4.Work\2.Gather data\FN009 Weights\y2000.csv
 4)files for the stage2 part (mod2) are created and are here:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.1.NE PM MODELS\3.1.1.4.Work\3.Analysis\AN003 mod2\y2000.csv

*** 12.create MPM for mod3

**** 1.assign mean PM to fullgrid

in GIS we load the unique PM station id (sitecode) from the output of
script cn002 located here:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.1.NE PM MODELS\3.1.1.4.Work\2.Gather data\FN007 Key tables\pm sites.dbf
 [[NE_PM_10x10_files/img/SS-2011-10-27_11.33.48.png]]
 then using spatial join we assign each sitecode the region it falls in
resulting in gn030 sitecode region
 [[NE_PM_10x10_files/img/SS-2011-10-27_11.37.31.png]]
 this is exported to dbf in the key table folder :

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.1.NE PM MODELS\3.1.1.4.Work\2.Gather data\FN007 Key tables\pmcode region.dbf

**** 2.assign mpm to mod2/mod3

using script c012 create mpm we:
 1)create a mpm (mean PM) file for each year for every day in every
region outputed here:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.1.NE PM MODELS\3.1.1.4.Work\2.Gather data\FN013 mpm peryear\mpm2000.sas7bdat
 *NOTE: before running this script make sure you have already run mod2
so that the mod2 files are available*
 2)assign mpm per region for a day for *mod2 prediction* file and output
here:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.1.NE PM MODELS\3.1.1.4.Work\3.Analysis\AN004 mod2pred\T2000 m2 pred mpm.csv
 3)assign mpm per region for a day for *mod3*prediction file and output
here:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.1.NE PM MODELS\3.1.1.4.Work\3.Analysis\AN005 mod3\fullgrid mpm 2000.csv

**** 3.2.assign mpm to mod2/mod3 CV

using script c013 create mpm CV MACRO we take the created files from
:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.1.NE PM MODELS\3.1.1.4.Work\2.Gather data\FN013 mpm peryear
 and give the 90% CV data files (per year at > AN009 mod2 CV files)
the mean pm in each region and output it here:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.1.NE PM MODELS\3.1.1.4.Work\3.Analysis\AN009 mod2 CV files mpm\

*** 13.prepare data for residual localpm part

**** 1.create 300 meter buffers from stations

a PM Id layer with all pm stations is loaded
 then a 300 m buffer is created around that
 [[NE_PM_10x10_files/img/SS-2011-08-05_13.16.06.png]]

**** 2.elevation

data for elevation on a 250m resolution was obtained from mi-hye (SEEK
REFERENCE)
 then each 300 meter buffer was calculated the mean elevation:
 [[NE_PM_10x10_files/img/SS-2011-08-05_13.27.41.png]]
 *
 NOTE: since Each variable is called RASTERVALUE, gis wont be able to
extract values for another variable with the same name (it will give all
values a NULL value). To address this simple create a new variable ,
copy the raster value to it and delete the raster value field
*then the -9999 values are changed to '0'*:
 [[NE_PM_10x10_files/img/SS-2011-08-05_13.37.27.png]]
*this files is saved as pd elevations

**** 3.calculate traffic density

 4. Traffic Density

 - In ArcToolbox, double-click the clip tool and clip the roads by
buffers

 [[NE_PM_10x10_files/img/Image.jpg]]

 then using the non toolbox spatial join option each buffer of
monitoring stations was given the sum of all cliped roads intersecting
it

 [[NE_PM_10x10_files/img/SS-2011-08-08_10.38.09.png]]

**** 4.join elev and traffic to buffers

using 2 simple spatial joins by closest the 2 above layers with elev and
total length of roads in each buffer (tden) was joined to the buffer
layers
 results in:
 rn6 join2 road

**** 5.land cover

**

To calculate land cover (Percentage of urbaness)

 - Add the binary raster of urbaness(refer to Itai's email on criteria
on reclassification of NLCD)

 [[NE_PM_10x10_files/img/Image_a3HNT75ALE9hgMLbQkyqVQ_0001.jpg]]

 - Run the '*zonal statistics as table*' and select mean (mean for
binary data is percentage)
 NOTE: if the procedure fails try to save it outside of a gdb as a
normal table in a folder

 [[NE_PM_10x10_files/img/SS-2011-08-08_10.40.40.png]]

 This results in Percentage of open space(technically, percentage of
urbaness)
 [[NE_PM_10x10_files/img/Image_a3HNT75ALE9hgMLbQkyqVQ_0002.jpg]]

 then using a normal join it was added back to the *rn6 join2* road
layer based on sitecode:
 [[NE_PM_10x10_files/img/SS-2011-08-08_10.43.57.png]]

**** 6.population density

data was obtained through steve from the lanscan data in raster form
lanscan at a 1x1km resolution
 first using extract to point we extracted the population values to the
monitor points (centroid of the 300m buffers).
 [[NE_PM_10x10_files/img/SS-2011-08-09_12.04.00.png]]
 this layer rn7 was then joined to the *rn6 join2*layer and the key
variables only left and renamed in the proccess.
 this results in r*n8
*then we calcualted the pop density by deviding the population by the
area of the buffer (*note: the layer had to be reprojected to utm 18n so
that we could calculate area*)
 *
**
*

**** 7.final data

the final data including all the LU 50x50 terms is located here:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.1.NE PM MODELS\3.1.1.4.Work\3.Analysis\AN002x mod1 localpm\resid lu.csv

*** 15.cluster analysis

**** intro- testing the cluster method

We deciided to test a alternative method to using the bimon way
(essentially using a cluster for every 2 months per year) and to use 9
clusters based on PM levels in each day for the whole period

**** create all year pm dataset for the cluster analysis

using *c015 prepeare pm dataset forcluster*we prepared a data set
with all years resulting with the *PM* for every*station* in each *day*
 [[NE_PM_10x10_files/img/SS-2011-11-17_11.14.13.png]]
 in the script stations missing PM for a specific day get the PM from
the next closets (by distance) PM station.
 If a station is still missing it gets the PM from the meanPM file we
create in step 12
 lastly the data is transposed and outputted here:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs3.1.1.NE PM MODELS2\3.1.1.4.Work\3.Analysis\AN030 cluster\pmcluster.csv
 In addition we tested how the cluster preforms every 3 years instead of
1 big yearly file. the big file from above was spllit to 3 3year files
outputted here:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.1.NE PM MODELS\3.1.1.4.Work\3.Analysis\AN030 cluster\pmcluster0002.csv

*** 16. PM 10 comparison

**** 1.merge PM data with PM2.5

use script *cn024*

** 3.Analysis
*** 1.Stage 1  (mod1)
**** 1.split files and add weights

using the cn019 split4cv script in R a weight variable is added to the
aod-pm-lu-met dataset to take into account the larger number of summer
days over winter days
 We incorparate all avilable covariates in the model using a model that
emulates a 'kernel machine'
 each spatial and temporal covariate is transformed to a Z-score by
subtracting the *Mean* from each obs. and deviding by the *SD*:
 example:
 F T2001 All$zelev<-(F T2001 All$CONTOUR-*97.22*)/*165.32*
 and the files are split randomly into the 10% files and 90% files
 this also creates the files needed for the stage2 splits
 *output:*

c:/Users/ekloog/Documents/$Doc/3.PostDoc/3.1.Projetcs/3.1.1.NE PM MODELS/3.1.1.4.Work/3.Analysis/AN002 mod1 CV/out2000.dbf
 *and for mod 2:
*

c:/Users/ekloog/Documents/$Doc/3.PostDoc/3.1.Projetcs/3.1.1.NE PM MODELS/3.1.1.4.Work/3.Analysis/AN003 mod2/sas/out2001.dbf

**** 2.run mod1 for full dataset

Using sas scripts*c020 mod1 100percent* a mixed model is run in SAS to
calibrate the ground station PM with AOD including spatial and temporal
variables. the outputs of the model is here:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.1.NE PM MODELS\3.1.1.4.Work\3.Analysis\AN002 mod1 CV\overall random\
 *NOTE:*when the covariance paramters are not '0' we get the estimation
for all the random effects. Since the model is very complex we are
missing some covariance parameters for the day-region combination (for
example the random slope for Temp per region)
 therefore we use the output of the 100% dataset (full) and take the
covariance parameter estimates to be used in the*CV iterations model*
 this could happen in the 100% dataset as well so you use the same
method in the model there.
 [[NE_PM_10x10_files/img/SS-2011-10-26_10.13.58.png]]
 [[NE_PM_10x10_files/img/SS-2011-10-26_10.31.18.png]]

**** 3.Run first mixed models and CV (mod1)

after adding the boundaries from the full model (prev step) to the
individual sas scripts*c020macro mod1CV 2000 to
c020macro mod1CV 2008*we use a batch file *c020macro mod1CV BATCH*
to run CV for all years.
 The CV mixed models predict for the 10% files from the 90% files
 the files for each year are aggregated to one large file and outputted
here:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.1.NE PM MODELS\3.1.1.4.Work\3.Analysis\AN002 mod1 CV\sas
export\t2008 all 10p.csv
 *NOTE: we also used script* *c020 mod1 CV R
*c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.1.NE PM MODELS\3.1.1.3.Code\Archive\$brents
R method for CV\c020 mod1 CV R.R to compare the SAS results with R and got the same results

**** 4.add local PM stage for CV results

in addition we add the local pm (resid) part to this script
*c020 mod1 100percent localPM
*

*** 2. Stage 2

**** 1.Run second mixed model (mod2)

using *c021 mod2* a macro is run and the fit from the full step 1 model
(mod1) is used to predict PM for the mod2 files (ALL avilable AOD
points)
 the files with predictions are located here:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.1.NE PM MODELS\3.1.1.4.Work\3.Analysis\AN004 mod2pred\T2000 m2 pred.dbf
 after this *run the MPM stage to get mpm for all these predictions*

**** 2.run second stage cross validation

using script*c032 mod2 CV MACRO.sas* the CV files for mod2 are
prepared

*** 3. Stage 3

**** 5.Run GAMM stage model (mod3)

using *c023 gamm* we run the GAMM stage. we then predict for the all
points (all days in all the grid).
 the files are split into the prediction files from the lme outputted
here as dbf:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.1.NE PM MODELS\3.1.1.4.Work\3.Analysis\AN006 mod3pred\grid 2000 bimon1.dbf
 and the prediction part from the smoothing outputted as csv here:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.1.NE PM MODELS\3.1.1.4.Work\3.Analysis\AN006 mod3pred\T2000 bimon1.csv

**** 5.Run CV GAMM (mod3 CV)

using *c032 gamm all CV* we run the GAMM CV stage
 the outputs are saved in the results file for all years here:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.1.NE PM MODELS\3.1.1.5.Results\mod3cv\mod3CV r2.csv

*** 4.create full poll dataset

**** 1.merge all predcitions

using script *c039 create poll datasets.sas*we :
 2) combine all datasets into 1 (by guid and date)
 3)choose the PM predicted value (pmnew) according to the best avilable
 NOTE: though initially we used actual PM for best then mod2 and finally
mod3, in the Final dataset we just used mod2+mod3 since actual pm from
stations could be biased from lets say a major highway close by to a
station and thus we decided to use mod2+mod3 that cover a 10x10 grid and
not just a measurement location
 this was done with a "if then" statements (see script).
 this resulted in a data sets for pm2.5 predictions incorporating ALL
models for New-England for 2000-2008.

**** 2.create lag files

using script *cn60 add Temp and lags*
 we import the individual files, add them to a full GRID (so that
missing days in the very rurual areas get a missing values )
 we also import the temprature data
 we then run the lags and output the full 265 day lags and the one we
willa ctually work with (just the moving avergares) here:

c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.1.NE PM MODELS\3.1.1.4.Work\3.Analysis\AN040 Lags\poll lag.sas7bdat

*** 5.LPM map

**** 1.predict from pm stgations to full grid

using this script cn070 LPM 200x200 we calculate the local pm based on
the fit of the pm stations (resid~LU terms)
 this is exported here:

*c:\Users\ekloog\Documents\$Doc\3.PostDoc\3.1.Projetcs\3.1.2.MIA PM MODELSV3\3.1.1.4.Work\3.Analysis\AN070 LPM stage 200x200 base\lu200LPM.dbf*

**** 2.create raster in GIS

in GIS the DBF file of all LPM points and resid predictions is loaded
and using the
 [[NE_PM_10x10_files/img/Image_U9VdxiqvpVxcUUWkBXKCAQ_0001.png]]
 we convert the files into a rater to present as a map

** 4.post analysis

*** 1.check bias from extremely low pop density monitors

when we checked the slope of the actuall monitors vs our predicted data
using this script cn52 check spatial correlation we found that some
years had a very high slope (around 1.7)
 when we plotted this
[[NE_PM_10x10_files/img/Image_iAyxYcQL1R3X98P99ojTuw_0001.png]]on a
scatter plot:
 we saw that the points the were pulling the regression line were the
points in which the true PM (pm25) were very low (< 6)
 we therefore exported these station only to dbf and plotted them in
ArcGIS (the green triangles marked
in[[NE_PM_10x10_files/img/Image_iAyxYcQL1R3X98P99ojTuw_0002.png]] red in
the photo)
 these station were mostly IMPROV station where no one lived
 we also saw that the all have extremely low population density (the
black grids)
