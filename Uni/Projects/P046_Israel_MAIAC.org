 #+TITLE:P046_Israel_MAIAC 

* 0.Code run order
** Matlab run
*** base scripts 

before you start the following files should be in your matlab script dirs

file:/media/NAS/Uni/org/files/Uni/Projects/code/P046.Israel_MAIAC/MAIAC-Matlab/jl2normaldate.m
file:/media/NAS/Uni/org/files/Uni/Projects/code/P046.Israel_MAIAC/MAIAC-Matlab/julian2date.m
file:/media/NAS/Uni/org/files/Uni/Projects/code/P046.Israel_MAIAC/MAIAC-Matlab/MCreadv3.m
file:/media/NAS/Uni/org/files/Uni/Projects/code/P046.Israel_MAIAC/MAIAC-Matlab/MCrunv3.m

$Note$ the first 2 are matlab functions the last 2 (scripts ) use

-create a text file with all your hdf files and full PATH
-create a lat and long file hdf files

the code is devided into two main files:

maiacrun- the file you run which calls the function
maiacread-the function itselfs with most of the code

The code itself is commented so more datailed explenations on ecah step can be viwed in the code but a few important points:

you need to change the matlab PATH to where the function is. this can be done via the gui or with the following command:

#+BEGIN_SRC octave 
%change dir to path
cd '/home/zeltak/smb4k/ZUNISYN/ZUraid/Uni/org/files/Uni/Projects/code/P46/MAIAC'
#+END_SRC

also, make sure that the file names are correct for the function, since if you rename it the script will crash and give you an error

@note: an error may occur when trying to extract the dates from the file names, make sure terra is always written as 'tera' in all occurances (paths names, text file with paths etc) so that the terra script part dosent crash@

** R scripts
***  002-clean AOD
this script
file:/media/NAS/Uni/org/files/Uni/Projects/code/P046.Israel_MAIAC/p002_aod_clean.r

will load the matlab output for AOD ,clean the data , run EDA etc and output full and yearly RDS files outputed here:

file:/media/NAS/Uni/Projects/P046_Israel_MAIAC/3.Work/2.Gather_data/FN003_AOd_allyears/allaodAQ2002.rds
*** 004-clean station data and join to AOD
 
file:/media/NAS/Uni/org/files/Uni/Projects/code/P046.Israel_MAIAC/p004_stn_clean.r

We use this script to import all avilable israeli station data (PM,Met,co-pollutants etc)
we clean, run EDA import LU and join to AOD
this results in 2 all years files outputed for PM25 and PM10:
mod1.pm10all.RDS
mod1.pm25all.RDS

and per year files as well

*** 005 run mod1

file:/media/NAS/Uni/org/files/Uni/Projects/code/P046.Israel_MAIAC/p005_mod1.r

* 1.Data info
** 1.Air tempreature data
*** 1.data sources
data was obtained through david broday and is part of the pm-other monitors
$ask meeytr about more info RE this$

** AOD
*** add x,y ITM cords to aod points (in meters)

#+DOWNLOADED: file:///media/NAS/Uni/org/attach/images_2014/Screenshot%20-%2002242014%20-%2010:08:19%20AM.png @ 2014-02-24 10:08:45
#+attr_html: :width 300px
 [[/media/NAS/Uni/org/attach/images_2014/Screenshot%20-%2002242014%20-%2010:08:19%20AM_2014-02-24_10:08:45.png]]

** 2.MODIS Surface temp data and Emissivity
*** 1.info
**** Sinsoudial projection

%TIP%- To manually convert points (such as corners X,Y) from sinsoudial to WGS84 lat long one can use the online calculator

http://landweb.nascom.nasa.gov/cgi-bin/developer/tilemap.cgi

Almost all of the fine resolution L3 land products are made in the Sinusoidal Grid. The L3 fine resolution Sea-ice products are the only exception and are made in the in the Lambert Azimuthal Equal-Area projection with the grid centered at the north and south poles. For Collection 3 and before the Sinusoidal Grid products were made in the Integerized Sinusoidal Grid.
The grid cell size varies by product and is either (approximately) 0.25 km, 0.5 km or 1 km. The actual size depends on the projection.
Each grid is broken into a number of non-overlapping tiles which cover approximately 10x10 degree area. The tiles are labeled with a vertical and horizontal tile id.

In additional, the MODLAND coarse resolution global Climate Modeling Grid (CMG) products will be made in a geographic projection. The grid cell sizes for these products are 0.05 degrees.

There are 460 non-fill 10 deg. by 10 deg. tiles in the grid.
The tile coordinate system starts at (0,0) (horizontal tile number, vertical tile number) in the upper left corner and proceeds rightward (horizontal) and downward (vertical). The tile in the bottom left corner is (35, 17).
Dark blue tiles contain only water (no land).
Tile system in black and white.
The boundaries of tiles outside of the grid's image area are not shown.
Table of Tile Bounding Coordinates (10 deg tiles).
Table of Tile G-ring Coordinates (10 deg tiles).
Projection parameters for the General Cartographic Transformation Package (GCTP).

**** conversions
kelvin to farenhight GIS:
 ((([tempk]-273) * 1.8)+32)

**** general
This is the site with the MODIS surface temprature data:

[[https://lpdaac.usgs.gov/products/modis_products_table][https://lpdaac.usgs.gov/products/modis\_products\_table]]
look at the MODIS Products Table:
[[
MIA_NE_temp_models_files/img/Image_Dk9i5VXLnkjjdxxWji1MDQ_0001.png]]


We want to use the *MOD11A1* data which is at a 1x1km resolution daily, and should have 2 separate reading for day and night
we can also check the MODIS Overview page to see which tiles we are interested in

**** scale factor
Note: when calculating final temperature/emmisivty you must apply the
scale factor.
For temp: you multiple by 0.02
For emissivty you multiple by
0.02+0.49.


see sas code cn004\_modis\_import.sas for more detail
[[
MIA_NE_temp_models_files/img/SS-2011-04-26_15.27.46.png]]
[[
MIA_NE_temp_models_files/img/Image_owwqUrxD7Lyx2ey2prMabQ_0001.png]]

**** MODIS Tiling system

[[file:MIA_NE_temp_models_files/img/sn_10deg.gif]]

[[MIA_NE_temp_models_files/img/Image_Ti5oxlPHCjx2K4Qxex3RAw_0001.png]]

**** Naming Scheme of Files

MODIS Naming Conventions

MODIS filenames (i.e., the local granule ID) follow a naming convention
which gives useful information regarding the specific product.
For example, the filename

MOD09A1.A2006001.h08v05.005.2006012234657.hdf
indicates:

MOD09A1 - Product Short Name
.A2006001 - Julian Date of Acquisition (A-YYYYDDD)
.h08v05 - Tile Identifier (horizontalXXverticalYY)
.005 - Collection Version
.2006012234567 - Julian Date of Production (YYYYDDDHHMMSS)
.hdf - Data Format (HDF-EOS)
The MODIS Long Name (i.e., Collection

%rename in total commander%:
[N9-16]\_[N19-23]

**** modis data projections
[[MIA_NE_temp_models_files/attach/MODIS%20Projection.prj][Attachment #01
(MODIS Projection.prj)]]
 see attached file for a prj arcgis file for modis data projection

*** 2.download from modis website
**** use REVERB
We used the Reverb site to download the MODIS surface temperature data
The method is as follows:

first choose a Geo aream choose the dates and sepcific dataset (we use MOD11A1)

[[file:1.Images/2542013w.png]]
then in the following screen you add all the selected items to the cart

[[file:1.Images/2542013w2.png]]

then press the Download button

[[file:1.Images/2542013w3.png]]

and choose the following options


[[file:1.Images/2542013w4.png]]

This is then used to dowload the data on a Linux machine using Wget
First concatanate all yearly files into one file, ₆In example₆ 

wget FILE1.TXT 


#+BEGIN_SRC sh
wget -i FILE.txt
#+END_SRC


#+BEGIN_SRC sh
#where file.txt is a simple text file pointion at all the seperate ST.txt batch file IE:
~/Downloads/LST_ISRAEL/data2005_url_script_2013-04-15_103101.txt
~/Downloads/LST_ISRAEL/data2010_url_script_2013-04-15_103101.txt
~/Downloads/LST_ISRAEL/data2011_url_script_2013-04-15_103101.txt

#+END_SRC

this will download all the hdf files (and other files with *.xml,*.jpeg which you can discard)

*** 3.HDF to DBF
**** Convert using matlab
***** extract lan and long for each tile
As a first step we need to extract the fixed lat and long for each tile. we use hdf dump for that:

http://hdfeos.org/software/eosdump.php

you need to download the binary, or use attached file:

[[file:MIA_NE_temp_models_files/attach/eos2dump][EOSdump]]

while the basic command is run from a bash shell on linux

#+BEGIN_SRC sh
./eos2dump -c -1  h18v04.hdf  > lat_h18v04.output
./eos2dump -c -2  h18v05.hdf  > long_h18v05.output
#+END_SRC

where -1 reffers to lat extraction and -2 to long extraction

Save the lat/long file for each tile and move them back to a windows machine

$NOTE: you can also create a simple batch files to run the above commands in batch for multiple tiles$

***** Extract in matlab to dbf
****** Original Nasa script
we base our scirpts partly on this script:

http://www.hdfeos.org/zoo/LPDAAC_MOD_matlab.php

#+BEGIN_SRC octave "Original matlab code"

% Copyright (C) 2012 The HDF Group
% All rights reserved.
%
%   This example code illustrates how to access and visualize LP_DAAC
% MOD Sinusoidal Grid file in Matlab.
%
%   If you have any questions, suggestions, comments on this
% example, please use the HDF-EOS Forum  (http://hdfeos.org/forums).

%   If you would like to see an  example of any other NASA
% HDF/HDF-EOS data product that is not listed in the HDF-EOS
% Comprehensive Examples page (http://hdfeos.org/zoo),
% feel free to contact us at eoshelp@hdfgroup.org or post it at the
% HDF-EOS Forum (http://hdfeos.org/forums).

clear

% Define file name, grid name, and data field.
FILE_NAME='MOD11A1.A2012275.h28v04.005.2012276122631.hdf';
[PATHSTR, BASE_NAME, EXT]=fileparts(FILE_NAME);
GRID_NAME='MODIS_Grid_Daily_1km_LST';
DATAFIELD_NAME='LST_Night_1km';

% Open the HDF-EOS2 Grid file.
file_id = hdfgd('open', FILE_NAME, 'rdonly');

% Read data from a data field.
grid_id = hdfgd('attach', file_id, GRID_NAME);


[data1, fail] = hdfgd('readfield', grid_id, DATAFIELD_NAME, [], [], []);

% Convert the data to double type for plot.
data2=double(data1);


% This file contains coordinate variables that will not properly plot.
% To properly display the data, the latitude/longitude must be remapped.

[xdimsize, ydimsize, upleft, lowright, status] = hdfgd('gridinfo', grid_id);

% Detach from the grid object.
hdfgd('detach', grid_id);

% Close the File.
hdfgd('close', file_id);

% The file contains SINUSOIDAL projection. We need to use eosdump to
% generate 1D lat and lon and then convert them to 2D lat and lon accordingly.
% To properly display the data, the latitude/longitude must be remapped.
% For information on how to obtain the lat/lon data, check [1].
lat1D = load(['lat_' BASE_NAME '.output']);
lon1D = load(['lon_' BASE_NAME '.output']);

lat = reshape(lat1D, xdimsize, ydimsize);
lon = reshape(lon1D, xdimsize, ydimsize);

clear lat1D lon1D;

lat = lat';
lon = lon';

% Read attributes from the data field.
SD_id = hdfsd('start', FILE_NAME, 'rdonly');

sds_index = hdfsd('nametoindex', SD_id, DATAFIELD_NAME);

sds_id = hdfsd('select',SD_id, sds_index);

% Read filledValue from the data field.
fillvalue_index = hdfsd('findattr', sds_id, '_FillValue');
[fillvalue, status] = hdfsd('readattr',sds_id, fillvalue_index);

% Read units from the data field.
units_index = hdfsd('findattr', sds_id, 'units');
[units, status] = hdfsd('readattr',sds_id, units_index);

% Read scale_factor from the data field.
scale_index = hdfsd('findattr', sds_id, 'scale_factor');
[scale, status] = hdfsd('readattr',sds_id, scale_index);

% Read long_name from the data field.
long_name_index = hdfsd('findattr', sds_id, 'long_name');
[long_name, status] = hdfsd('readattr',sds_id, long_name_index);

% Read valid_range from the data field.
valid_range_index = hdfsd('findattr', sds_id, 'valid_range');
[valid_range, status] = hdfsd('readattr',sds_id, valid_range_index);


% Terminate access to the corresponding data set.
hdfsd('endaccess', sds_id);
% Close the file.
hdfsd('end', SD_id);

% Replace the filled value with NaN.
data2(data2 == fillvalue) = NaN;

% Process valid_range.
data2(data2 < valid_range(1)) = NaN;
data2(data2 > valid_range(2)) = NaN;

% Apply scale factor according to the field attribute LST.
% "LST: LST data * scale_factor".
data2 = data2 * scale;

% Transpose the data to match the map projection.
data=data2';

% Plot the data using contourfm and axesm.
latlim=[floor(min(min(lat))),ceil(max(max(lat)))];
lonlim=[floor(min(min(lon))),ceil(max(max(lon)))];
min_data=min(min(data));
max_data=max(max(data));

% Create the figure.
f=figure('Name', FILE_NAME, 'visible', 'off');

% We need finer grid spacing since the image is zoomed in.
% MLineLocation and PLineLocation controls the grid spacing.
axesm('MapProjection','sinusoid','Frame','on','Grid','on',...
      'MapLatLimit',latlim,'MapLonLimit',lonlim, ...
      'MeridianLabel','on','ParallelLabel','on', ...
      'MLabelLocation', 5, 'PLabelLocation', 5)
coast = load('coast.mat');

surfacem(lat,lon,data);
colormap('Jet');
caxis([min_data max_data]);

% Change the value if you want to have more than 10 tick marks.
ntickmarks = 10;
granule = (max_data - min_data) / ntickmarks;
h = colorbar('YTick', min_data:granule:max_data);

set (get(h, 'title'), 'string', units, 'FontSize',16,'FontWeight','bold');

plotm(coast.lat,coast.long,'k')

title({FILE_NAME; long_name}, 'Interpreter', 'None', ...
    'FontSize',16,'FontWeight','bold');

% The following fixed-size screen size will look better in JPEG if
% your screen is too large. (cf. scrsz = get(0,'ScreenSize');)
scrsz = [1 1 800 600];
set(f,'position',scrsz,'PaperPositionMode','auto');
saveas(f, [FILE_NAME '.m.jpg']);

% Reference
%
% [1] http://hdfeos.org/zoo/note_non_geographic.php
% [2] http://modis-sr.ltdri.org/products/MOD09_UserGuide_v1_3.pdf
#+END_SRC

****** Our Script (andrea padoan)
We had help from a Matlab programmer (andrea.padoan@unipd.it)

There are a series of scripts and functions that are run to get
Txtfile from the HDF files, attached here as ZIP:

[[file:2.Attach/extract_hdf.zip][Extract_hdf_matlab_scripts]]

!folder structure!

the hdf files must be split into yearly folders 2000-2011, and in
each folder there should be a copy of all the tiles lat/long files
and scripts

We run the Batch scripts that goes through all yearly folder but the
main script is the HDF_import script

the script needs to be changed inside year to year in each folder, IE
change 2000>2011

#+BEGIN_SRC octave


function HDF_Import (whatimport)
%% Import HDF image file and export lat, long, Day, Night, Emis and Reference day
% from a list of HDF image files and a series of lat and long files.
% Lat and long files must be as vectors.
% Type:
% HDF_Import ('stage1') or HDF_Import ('stage2') or HDF_Import ('stage3')
% or HDF_Import ('stage4') to perform single geographical block analysis or
% HDF_Import ('all') to perform all analyses.
% Inpath and Outpath should be modified and setted to reflect the file
% actual position.
% LEGEND:
% Stage1: h18v04 files and as output a file named OutputMergedh18v04.txt
% Stage2: h18v05 files and as output a file named OutputMergedh18v05.txt
% Stage3: h19v04 files and as output a file named OutputMergedh19v04.txt
% Stage4: h19v05 files and as output a file named OutputMergedh19v05.txt
%
% Legend to the main structure:
% HDF_I_S = struct('inpath', {'/Users/andrea/Documents/MATLAB/Itai/matlab/'}, ...
%                                   'outpath', {'/Users/andrea/Documents/MATLAB/Itai/New2/Output/'}, ...
%                                   'codepath', {'/Users/andrea/Documents/MATLAB/Itai/New2'}, ...
%                                   'lat1', {'lat_h18v04.output'},'long1', {'long_h18v04.output'}, ...
%                                   'lat2', {'lat_h18v05.output'},'long2',{'long_h18v05.output'}, ...
%                                   'lat3', {'lat_h19v04.output'},'long3',{'long_h19v04.output'}, ...
%                                   'lat4', {'lat_h19v05.output'},'long4',{'long_h19v05.output'}, ...
%                                   'stage1', {'*h18v04*.hdf'}, 'stage2', {'*h18v05*.hdf'}, ...
%                                   'stage3', {'*h19v04*.hdf'}, 'stage4', {'*h19v05*.hdf'}, ...
%                                   'stages', {4}, 'FileExtension', {'*.hdf'}, ...
%                                   'OutFileName1', {'OutputMergedh18v04.txt'}, ...
%                                   'OutFileName2', {'OutputMergedh18v05.txt'}, ...
%                                   'OutFileName3', {'OutputMergedh19v04.txt'}, ...
%                                   'OutFileName4', {'OutputMergedh19v05.txt'});


    if (nargin < 1)
    fprintf ('\n%s\n', 'This function need some parameters.');
        return;
    end
    %My path
    HDF_I_S = struct('inpath', {'f:\Uni\Projects\P020_Temprature_ITALY\1_Raw_data\MODIS\T2011\'}, ...
                                  'outpath', {'f:\Uni\Projects\P020_Temprature_ITALY\1_Raw_data\MODIS\T2011\Output\'}, ...
                                  'codepath', {'f:\Uni\Projects\P020_Temprature_ITALY\1_Raw_data\MODIS\T2011\'}, ...
                                  'lat1', {'lat_h18v04.output'},'long1', {'long_h18v04.output'}, ...
                                  'lat2', {'lat_h18v05.output'},'long2',{'long_h18v05.output'}, ...
                                  'lat3', {'lat_h19v04.output'},'long3',{'long_h19v04.output'}, ...
                                  'lat4', {'lat_h19v05.output'},'long4',{'long_h19v05.output'}, ...
                                  'stage1', {'*h18v04*.hdf'}, 'stage2', {'*h18v05*.hdf'}, ...
                                  'stage3', {'*h19v04*.hdf'}, 'stage4', {'*h19v05*.hdf'}, ...
                                  'stages', {4}, 'FileExtension', {'*.hdf'}, ...
                                  'OutFileName1', {'OutputMergedh18v04.txt'}, ...
                                  'OutFileName2', {'OutputMergedh18v05.txt'}, ...
                                  'OutFileName3', {'OutputMergedh19v04.txt'}, ...
                                  'OutFileName4', {'OutputMergedh19v05.txt'});

    %Create the structure for the edges to trim the blocks.
     HDF_I_Edges = struct ('latup', 47.4, 'latlow', 36.5, 'longup', 18.6, 'longlow', 6.4);


    %Original coordinated given by Itai and Francesco
    % longTu = 19;
    %     longTl = 6;
    %     latTu = 48;
    %     latTl = 36;

    %Latitute and Longitude structure
    HDF_I_geog = struct('lat', {}, 'long', {}, 'stage', {});

    %Create the output directory if it doesnt' exist
    if (~isdir(HDF_I_S.outpath))
        mkdir(HDF_I_S.outpath);
    end

    path(path, HDF_I_S.codepath);

    %Change the working dir
    chdir(HDF_I_S.inpath);

    %Allocate space
    for i =1:1:HDF_I_S.stages
        HDF_I_geog(i).lat= zeros(1440000,1);
        HDF_I_geog(i).long= zeros(1440000,1);
    end


    %% Perform stage 1
    % Retrieve dir list of files, with the extension you gave
    %
    if (strcmpi(whatimport,'stage1') | strcmpi(whatimport,'all'))

        clear dirlist NumberOfFiles sYear hugeM

        %Change the working dir
        chdir(HDF_I_S.inpath);

        dirlist =dir(HDF_I_S.stage1);

        %Count the number of files in the current directory
        NumberOfFiles=size(dirlist, 1);

        if (NumberOfFiles ~= 0)

            fprintf('\n\nLoading stage 1 coordinates ....\n')
            %Load latituge and longitude in memory
            HDF_I_geog(1).lat = load(HDF_I_S.lat1);
            HDF_I_geog(1).long = load(HDF_I_S.long1);

            %Obtain all the files al load it in memory (sYear structure)
            [hugeM]= RetrieveAllYear (HDF_I_S.stage1, HDF_I_Edges, HDF_I_geog(1).lat,  HDF_I_geog(1).long);

%             %Internal check
%             if NumberOfFiles ~= length(sYear)
%                 error('Missmatch in file and Structure loaded in memory');
%             end
%
%             %Define the first matrix
%             hugeM = sYear(1).Data;
%
%             %Concatenate matrix
%             for Index=2:1:length(sYear)
%                 hugeM = [hugeM; sYear(Index).Data];
%             end
            tic;
            %Save the Merged file
            SaveHugeFile([HDF_I_S.outpath HDF_I_S.OutFileName1], hugeM);

            fprintf('\nOutputfile : %s\n', [HDF_I_S.outpath HDF_I_S.OutFileName1]);
            toc
        else
            %Display that no the has been processed
            fprintf('\n\nNo files to process in stage1....\n\n')

        end

        %Chech for the 'single' class
        if max(max(hugeM(:,1:5))) > 999
            warning('Class single for hugeM is not enought. Consider double');
        end

    end

    %% Perform stage 2
    % Retrieve dir list of files, with the extension you gave
    %
    if (strcmpi(whatimport,'stage2') | strcmpi(whatimport,'all'))

        clear dirlist NumberOfFiles sYear hugeM

        %Change the working dir
        chdir(HDF_I_S.inpath);
        dirlist =dir(HDF_I_S.stage2);

        %Count the number of files in the current directory
        NumberOfFiles=size(dirlist, 1);

        if (NumberOfFiles ~= 0)

            fprintf('\n\nLoading stage 2 coordinates ....\n')
            %Load latituge and longitude in memory
            HDF_I_geog(2).lat = load(HDF_I_S.lat2);
            HDF_I_geog(2).long = load(HDF_I_S.long2);

            %Obtain all the files al load it in memory (sYear structure)
            [hugeM]= RetrieveAllYear (HDF_I_S.stage2,HDF_I_Edges,HDF_I_geog(2).lat,  HDF_I_geog(2).long);

%             %Internal check
%             if NumberOfFiles ~= length(sYear)
%                 error('Missmatch in file and Structure loaded in memory');
%             end
%
%             %Define the first matrix
%             hugeM = sYear(1).Data;
%
%             %Concatenate matrix
%             for Index=2:1:length(sYear)
%                 hugeM = [hugeM; sYear(Index).Data];
%             end
            tic;
            %Save the Merged file
            SaveHugeFile([HDF_I_S.outpath HDF_I_S.OutFileName2], hugeM);
            fprintf('\nOutputfile : %s\n', [HDF_I_S.outpath HDF_I_S.OutFileName2]);
            toc
        else
            %Display that no the has been processed
            fprintf('\n\nNo files to process in stage2....\n\n');

        end
        %Chech for the 'single' class
        if max(max(hugeM(:,1:5))) > 999
            warning('Class single for hugeM is not enought. Consider double');
        end

    end


    %% Perform stage 3
    % Retrieve dir list of files, with the extension you gave
    %
    if (strcmpi(whatimport,'stage3') | strcmpi(whatimport,'all'))


        clear dirlist NumberOfFiles sYear hugeM


        %Change the working dir
        chdir(HDF_I_S.inpath);
        dirlist =dir(HDF_I_S.stage3);

        %Count the number of files in the current directory
        NumberOfFiles=size(dirlist, 1);


        if (NumberOfFiles ~= 0)

            fprintf('\n\nLoading stage 3 coordinates ....\n')
            %Load latituge and longitude in memory
            HDF_I_geog(3).lat = load(HDF_I_S.lat3);
            HDF_I_geog(3).long = load(HDF_I_S.long3);

            [hugeM]= RetrieveAllYear (HDF_I_S.stage3,HDF_I_Edges,HDF_I_geog(3).lat, HDF_I_geog(3).long);

%             if NumberOfFiles ~= length(sYear)
%                 error('Missmatch in file and Structure loaded in memory');
%             end
%
%             hugeM = sYear(1).Data;
%
%             for Index=2:1:length(sYear)
%                 hugeM = [hugeM; sYear(Index).Data];
%             end
            tic;
            %Save the Merge
            fprintf('\nOutputfile : %s\n', [HDF_I_S.outpath HDF_I_S.OutFileName3]);
            SaveHugeFile([HDF_I_S.outpath HDF_I_S.OutFileName3], hugeM);
            toc
        else
            %Display that no the has been processed
            fprintf('\n\nNo files to process in stage3....\n\n');
        end
        %Chech for the 'single' class
        if max(max(hugeM(:,1:5))) > 999
            warning('Class single for hugeM is not enought. Consider double');
        end
    end


    %% Perform stage 4
    % Retrieve dir list of files, with the extension you gave
    %
    if (strcmpi(whatimport,'stage4') | strcmpi(whatimport,'all'))

        clear dirlist NumberOfFiles sYear hugeM

        %Change the working dir
        chdir(HDF_I_S.inpath);
        dirlist =dir(HDF_I_S.stage4);

        %Count the number of files in the current directory
        NumberOfFiles=size(dirlist, 1);

        if (NumberOfFiles ~= 0)

            fprintf('\n\nLoading stage 4 coordinates ....\n')
            %Load latituge and longitude in memory
            HDF_I_geog(4).lat = load(HDF_I_S.lat4);
            HDF_I_geog(4).long = load(HDF_I_S.long4);

            [hugeM]= RetrieveAllYear (HDF_I_S.stage4, HDF_I_Edges, HDF_I_geog(4).lat,  HDF_I_geog(4).long);

%             if NumberOfFiles ~= length(sYear)
%                 error('Missmatch in file and Structure loaded in memory');
%             end
%             tic;
%             hugeM = sYear(1).Data;
%
%             for Index=2:1:length(sYear)
%                 hugeM = [hugeM; sYear(Index).Data];
%             end
%             toc
            tic;
            %Save the Merge
            SaveHugeFile([HDF_I_S.outpath HDF_I_S.OutFileName4], hugeM);
            fprintf('\nOutputfile : %s\n', [HDF_I_S.outpath HDF_I_S.OutFileName4]);
            toc
        else
            %Display that no the has been processed
            fprintf('\n\nNo files to process in stage4....\n\n');
        end
        %Chech for the 'single' class
        if max(max(hugeM(:,1:5))) > 999
            warning('Class single for hugeM is not enought. Consider double');
        end
    end

    %End of the function
end



#+END_SRC

when the script finishes its run you should have text files in the
output folder for each year, a seperate text file per year, IE

f:\Uni\Projects\P020_Temprature_ITALY\1_Raw_data\MODIS\T2011\Output\OutputMergedh19v05.txt

**** OLD METHOD (ARCGIS python script)
Warning: there was a bug in the script that caused an offset in X,Y from
the original HDF cords. we adress this in script cn004\_modis\_import
where we fix the x,y
 This applies both to the ST and NDVI modis images
 using using this script
f:\Uni\Projects\P020\_Temprature\_NE\_MIA\2.Code\2.Gather\_data\cn\_001\_hdf2\_points.py
results in dbf files for every layer of the hdf file (day ST, night ST
and emissivty) and the X,Y
 the files are outputed here:

f:\Uni\Projects\P020\_Temprature\_NE\_MIA\3.Work\2.Gather\_data\FN001\_yearly\_ST\

** 3.Procces and Combine WU and NCDC data
*** WU data
using script:

f:\Uni\Projects\P020\_Temprature\_NE\_MIA\2.Code\2.Gather\_data\CN\_002\_WU\_import.sas

we import the WU yearly files and export them to a db file :

f:\Uni\Projects\P020\_Temprature\_NE\_MIA\3.Work\2.Gather\_data\FN002\_WU
yearly\NEMIA\_2000.dbf
%Note%: some later years 2005-2011 are HUGE and thus its better to run
the script one year at a time to save resources otherwise the HD will be
fileld with temp files

*** NCDC data and combine with WU

using script:

f:\Uni\Projects\P020\_Temprature\_NE\_MIA\2.Code\2.Gather\_data\CN\_003\_WU\_NCDC\_Combo.sas
 we import the NCDC yearly files, combine them with the WU and export
them here :

f:\Uni\Projects\P020\_Temprature\_NE\_MIA\3.Work\2.Gather\_data\FN003\_WUNCDC
yearly\met2000.dbf
 also we create keytable files for individual station location per year
and export them here:

f:\Uni\Projects\P020\_Temprature\_NE\_MIA\3.Work\2.Gather\_data\FN007\_Key\_tables\met\_full\_grid.dbf
 NOTE: we also take care in this script of duplicate station that arise
from WU stations that appear both on the NE and MIA dataset but are the
same station

** 5.Modis NDVI data
*** 1.info

[[MIA_NE_temp_models_files/img/SS-2011-04-25_14.32.00.png]]
NVDI- ranges from -1 to 1:
*(1)* means that there is alot of vegetation while *(-1)* means there
is no veg. (barren/asphalt) there is a negative association between
LST and NDVI because of the cooling effect of canopy

-The scalling factor from grid code to NVDI is 0.0001


[[MIA_NE_temp_models_files/img/SS-2011-04-25_15.01.34.png]]

*** 2.download from modis dataset

The NDVI (vegetation index) is downloaded exactly as the surface
temperature but using a different data set:
[[MIA_NE_temp_models_files/img/SS-2011-04-25_14.07.08.png]]

the raw files are located here after a renaming (see same as LST scheme):

f:\Uni\Projects\P020\_Temprature\_NE\_MIA\1\_Raw\_data\ndvi\mia\_ne\A2011152\_11v04.hdf

*** 3.import hdf to table
using script

f:\Uni\Projects\P020\_Temprature\_NE\_MIA\2.Code\2.Gather\_data\cn\_001\_ndvi2\_points.py

we transformed the hdf file to database tables located here:

f:\Uni\Projects\P020\_Temprature\_NE\_MIA\3.Work\2.Gather\_data\FN006\_NDVI\tblm\_2000032\_0.dbf

*** 4.create yearly datasets and keytable

using script
f:\Uni\Projects\P020\_Temprature\_NE\_MIA\2.Code\2.Gather\_data\CN\_004\_NDVI\_import.sas
 we created yearly NDVI files with a month variable:

f:\Uni\Projects\P020\_Temprature\_NE\_MIA\3.Work\2.Gather\_data\FN006\_NDVI\_yearly\ndvi2000.dbf
 we also created a keytable located here:

f:\Uni\Projects\P020\_Temprature\_NE\_MIA\3.Work\2.Gather\_data\FN007\_Key\_tables\NDVI\_fullgrid.dbf

*** creat ndvi ID
for NDVI we loaded a full year, create the ndviid by joining X,Y of ndvi in GIS
then on first year run only  we created a unique grid and export that to DBF
this is then added to the full LU-rid ID's dataset
** 6.models preperation
*** create road density
road density rasters were created based on a road polyline layer for israel:
file:/media/NAS/Uni/Data/GIS/Israel/Roadden/roads.shp
then we used the line density tool to create a road density raster
we used the number of lanes as a population field (weight field)

#+DOWNLOADED: file:///home/zeltak/ZH_tmp/Screenshot%20-%2002242014%20-%2012:36:16%20PM.png @ 2014-02-24 12:36:44
#+attr_html: :width 300px

 [[/media/NAS/Uni/org/attach/images_2014/Screenshot%20-%2002242014%20-%2012:36:16%20PM_2014-02-24_12:36:44.png]]
** Dust day data
X	Y	StationID	Year	Month	Day	Dust		
220935	633613	BIL	2002	1	6	1	6	211
220763	629391	EFR	2002	1	6	1	6	232

Last column the max PM value per day
** potential covariets

#+DOWNLOADED: file:///media/NAS/Uni/org/attach/images_2014/Screenshot%20-%2003112014%20-%2008:31:45%20AM.png @ 2014-03-11 08:32:46
#+attr_html: :width 800px
 [[/media/NAS/Uni/org/attach/images_2014/Screenshot%20-%2003112014%20-%2008:31:45%20AM_2014-03-11_08:32:46.png]]

see also escape project databook:

file:/media/NAS/Uni/org/attach/files_2014/ESCAPE_Exposure-manualv9.pdf
** Land cover data
*** info on data
We used the land cover data from the lamas for 2003 (updated partially in 2007)

From LAMAS:
LAND USE - DEFINITIONS AND EXPLANATIONS
SOURCE OF DATA: The information is based on processing and integration of a
variety of administrative sources that were received from government ministries and non-
government and private organizations.
The database used for the information on land use was prepared through the
Geographic Information System (GIS) of the Israeli Central Bureau of Statistics. The
project was funded by the Ministry of the Interior.
The information is up-to-date to the end of 2002, except for data on plantations and
fields.
Land use: ways in which man takes advantage of the land for activities such as:
construction, agriculture, forestation, industry, etc.
Land use relates to the situation today, as opposed to land allocation relating to future
use.
The information on land use is national and continuous for all the land in the country.
Information can be produced according to various geographic categories, such as local
authorities, regional councils and natural regions.
Land use can be divided into two main types: constructed area and open area.
Constructed area is cross-sectioned according to various functions: housing,
education, health and welfare, public services, culture and leisure, commercial,
industry and infrastructure, transportation and agricultural buildings.
Open area is cross-sectioned according to the following types: public open area, forests,
agriculture and other open areas.
1. Education –areas used for educational purposes: kindergartens, schools,
universities, community centers and yeshivas.
2. Health and Welfare –areas used for health and welfare purposes: sick funds,
hospitals, and geriatric day centers.
3. Public services –areas used for public services: emergency services, public
administration, religion.
4. Culture, leisure, recreation and sports –areas used for culture - such as theaters,
cinemas, museums, public libraries, zoos, archeological sites, etc.; for leisure and
recreation - hotels, hostels, restaurants, amusement parks, etc.; and for sports –
stadiums, swimming pools, etc.
5. Commerce –areas used for commercial purposes: malls and commercial centers.
6. Industry and Infrastructure – areas used for industrial purposes – such as
industrial zones, waste disposal sites, mining and quarrying areas; and for
infrastructure facilities – airports, sea ports, sewage treatment plants, reservoirs.
7. Transportation – areas used for transportation purposes – such as parking lots,
gas stations, railway stations, taxi stands, and central bus stations.
8. Agricultural structures – areas used for agricultural structures – such as
greenhouses and fish ponds.
9. Residential –areas used for residential purposes and unclassified built area.
10. Public open area – areas used for public purposes – such as gardens or beaches.
11. Forest – areas used for man-made forests and wild groves.
12. Plantations, orchards and olive groves – areas coveredor olive trees.
13. Cultivated fields – areas used for cultivated fields.
14. Other open area – unclassified areas.
by plantations, orchards
The code for land use appears in the landuse field (valu


#+DOWNLOADED: file:///home/zeltak/ZH_tmp/Screenshot%20-%2004032014%20-%2012:32:25%20PM.png @ 2014-04-03 12:32:41
#+attr_html: :width 300px
 [[/home/zeltak/org/attach/images_2014/Screenshot%20-%2004032014%20-%2012:32:25%20PM_2014-04-03_12:32:41.png]]

In 2007 the constructed areas (residential buildings and agricultural structures) in the
region that is outside localities in the Negev, were updated. Only residential buildings
and agricultural structures in regions in the layer, that were originally presented as open
spaces, were updated.
Technical details about the layer

Description: The land use layer is a national, continuous layer. It contains
information regarding 14 various land uses. Parts are constructed areas and parts
are open areas.
The name of the layer is land use.
The layer is in SHP format.
The layer is presented in the Transverse Mercator projection.
The MMG layer is up-to-date to the end of 2002. (It includes an update from
2007, of residential buildings and agricultural structures, in the region outside of
localities in the Negev).
The layer is polygonalic.
The code for land use appears in the landuse field (values 1 – 14).
*** Calculate the perecent of specific land use in each 1km grid
We used the national land cover polygon layer from LAMAS ([[*info%20on%20data][info on data)]] was proccesed and reclassified to 0,1 files
₆In example₆ we classified all the open spaces (code 10-16) into '1' and build areas (1-7) to '0'


#+DOWNLOADED: file:///home/zeltak/ZH_tmp/snapshot3.png @ 2014-04-03 13:00:25
#+attr_html: :width 300px
[[/home/zeltak/org/attach/images_2014/snapshot3_2014-04-03_13:00:25.png]]

resulting in:

#+DOWNLOADED: file:///home/zeltak/ZH_tmp/snapshot1.png @ 2014-04-03 12:39:32
#+attr_html: :width 300px
 [[/home/zeltak/org/attach/images_2014/snapshot1_2014-04-03_12:39:32.png]]

we created from the MAIAC centroids a voroni 1x1 polygon layer

then using zonal statistics as table the mean ratio between 0(build area) and 1 (open space) was calculated  (using the aodid variable as zone field) and outputed as a table 
$Note-the aodid field has to be text other wise there may be issues$ 

#+DOWNLOADED: file:///home/zeltak/ZH_tmp/snapshot2.png @ 2014-04-03 12:49:16
#+attr_html: :width 300px
 [[/home/zeltak/org/attach/images_2014/snapshot2_2014-04-03_12:49:16.png]]

this gave us the mean value in each aodid cell

later when we import into R it was multiplied by 100 to get the % open using field calculator:
#+BEGIN_EXAMPLE
[MEAN] * 100
#+END_EXAMPLE


We repeat this process for each land cover variable we want to try (see next section)
*** Other Land cover variables calculated
Developed, Open Space - Includes areas with a mixture of some constructed materials, but mostly vegetation in the form of lawn grasses. 

Developed, High Intensity - Includes highly developed areas where people reside or work in high numbers. Examples include apartment complexes, row houses and commercial/industrial


#+DOWNLOADED: file:///home/zeltak/ZH_tmp/snapshot4.png @ 2014-04-03 13:29:17
#+attr_html: :width 300px
 [[/home/zeltak/org/attach/images_2014/snapshot4_2014-04-03_13:29:17.png]]
 
* 3.Analysis
** Cross validation mod1
in this part using the scripts:

[[file:f:/Uni/Projects/P020_Temprature_NE_MIA/2.Code/3.Analysis/MOD1CV_DAY.r]]
[[file:f:/Uni/Projects/P020_Temprature_NE_MIA/2.Code/3.Analysis/MOD1CV_Night.r]]
[[file:f:/Uni/Projects/P020_Temprature_NE_MIA/2.Code/3.Analysis/MOD1CV_MIT.r]]

a ten folds cross validation is run for all years. we test 3 different models St-day, ST-night and a combined
'weighted' model (MIT). since all preform very similarly we choose the combined ST model
we also check the spatial Vs temporal aspect

** Calculate R2, create yearly temp map and cleanup final predictions
using this script:

[[file:f:/Uni/Projects/P020_Temprature_NE_MIA/2.Code/2.Gather_data/CNSQL_PART3_finalize.sas]]

we calculate R2 for mod3 for each year and in addition output yeraly
maps for each year

then using this script:

[[file:f:/Uni/Projects/P020_Temprature_NE_MIA/2.Code/2.Gather_data/CNSQL_PART4_cleanup.sas]]

we clean up the predcitions and output FINAL predictions



* 4.results

